{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Vakyansh Recipies to build Speech Recognition models Introduction Vakyansh aims to host the key essentials of Automatic Speech Recognition (ASR) technology, focusing on Indian languages. It is a resource that allows people to build applications that leverage speech recognition. The site will host open data for training ASR models, open source utilities and pipelines to train ASR models and open ASR models themselves. Vakyansh also hosts tools to contribute your voices to create a diverse open data repository of Indian voices to this end. This data will be available in an open manner for all to use. It is a resource that allows people to build applications that leverage speech recognition. Open Source Open sourcing the speech recognition technology empowers us to bring our languages to the core of our fundamentals. Vakyansh aims to open source the speech recognition models in various languages, the datasets collected through various channels and the linguistic utilities developed to process and clean the data and make it usable by speech recognition tools. The open source strategy will enable the various language communities, individuals and technologists, who are passionate about their language, to develop speech recognition applications, and integrate them in various domains for the good of the community. Getting started Vakyansh's developer documentation is meant for its adopters, developers and contributors. It would enable people to innovate and improve and build Speech Recognition Models in any language. Modeling Process Vakyansh Components 1. Data Collection Pipeline Pipelines to collect data in automated way for the language you want 2. Intelligent Data Pipeline Pipelines to transform raw data and prepare data for model training. They clean, process and balance data for model training 3. Model Training Pipeline Pipeline to build state of the art Speech Recognition Model using the data provided by Intelligent Data Pipeline 4. Crowdsourcing Platform Platform to record and validate voice data from various speakers. Vakyansh Technical Overview The Developer documentation provides you with a complete set of guidelines which you need to: Install Vakyansh Configure Vakyansh Customize Vakyansh Extend Vakyansh Contribute to Vakyansh","title":"Home"},{"location":"#welcome-to-vakyansh","text":"Recipies to build Speech Recognition models","title":"Welcome to Vakyansh"},{"location":"#introduction","text":"Vakyansh aims to host the key essentials of Automatic Speech Recognition (ASR) technology, focusing on Indian languages. It is a resource that allows people to build applications that leverage speech recognition. The site will host open data for training ASR models, open source utilities and pipelines to train ASR models and open ASR models themselves. Vakyansh also hosts tools to contribute your voices to create a diverse open data repository of Indian voices to this end. This data will be available in an open manner for all to use. It is a resource that allows people to build applications that leverage speech recognition.","title":"Introduction"},{"location":"#open-source","text":"Open sourcing the speech recognition technology empowers us to bring our languages to the core of our fundamentals. Vakyansh aims to open source the speech recognition models in various languages, the datasets collected through various channels and the linguistic utilities developed to process and clean the data and make it usable by speech recognition tools. The open source strategy will enable the various language communities, individuals and technologists, who are passionate about their language, to develop speech recognition applications, and integrate them in various domains for the good of the community.","title":"Open Source"},{"location":"#getting-started","text":"Vakyansh's developer documentation is meant for its adopters, developers and contributors. It would enable people to innovate and improve and build Speech Recognition Models in any language.","title":"Getting started"},{"location":"#modeling-process","text":"","title":"Modeling Process"},{"location":"#vakyansh-components","text":"","title":"Vakyansh Components"},{"location":"#1-data-collection-pipeline","text":"Pipelines to collect data in automated way for the language you want","title":"1. Data Collection Pipeline"},{"location":"#2-intelligent-data-pipeline","text":"Pipelines to transform raw data and prepare data for model training. They clean, process and balance data for model training","title":"2. Intelligent Data Pipeline"},{"location":"#3-model-training-pipeline","text":"Pipeline to build state of the art Speech Recognition Model using the data provided by Intelligent Data Pipeline","title":"3. Model Training Pipeline"},{"location":"#4-crowdsourcing-platform","text":"Platform to record and validate voice data from various speakers.","title":"4. Crowdsourcing Platform"},{"location":"#vakyansh-technical-overview","text":"The Developer documentation provides you with a complete set of guidelines which you need to: Install Vakyansh Configure Vakyansh Customize Vakyansh Extend Vakyansh Contribute to Vakyansh","title":"Vakyansh Technical Overview"},{"location":"CONTRIBUTING/","text":"Contributing to this project Contributors Agreement Introduction I don't want to read this whole thing I just have a question!!! How Can I Contribute? Reporting Bugs Suggesting Enhancements Creating Pull Requests Contributors Agreement By submitting patches to this project you agree to allow them to be redistributed under the project's license, according to the normal forms and usages of the open-source community. Introduction First off, thank you for considering contributing to this project. It's people like you that make it such a great tool. Following these guidelines helps to communicate that you respect the time of the developers managing and developing this open source project. In return, they should reciprocate that respect in addressing your issue, assessing changes, and helping you finalize your pull requests. This is an open source project and we love to receive contributions from our community \u2014 you! There are many ways to contribute, from writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests or writing code which can be incorporated into the main project itself. I don't want to read this whole thing I just have a question!!! We currently allow our users to use the issue tracker for support questions. But please be wary that maintaining an open source project can take a lot of time from the maintainers. If asking for a support question, state it clearly and take the time to explain your problem properly. Also, if your problem is not strictly related to this project we recommend you to use Stack Overlow instead. How Can I Contribute? Reporting Bugs Before creating bug reports, please check the existing bug reports as you might find out that you don't need to create one. When you are creating a bug report, please include as many details as possible. How Do I Submit A (Good) Bug Report? Bugs are tracked as GitHub issues . Create an issue on the project's repository and provide the following information. Explain the problem and include additional details to help maintainers reproduce the problem: Use a clear and descriptive title for the issue to identify the problem. Describe the exact steps which reproduce the problem in as many details as possible. For example, start by explaining how you used the project. When listing steps, don't just say what you did, but explain how you did it . Provide specific examples to demonstrate the steps . It's always better to get more information. You can include links to files or GitHub projects, copy/pasteable snippets or even print screens or animated GIFS. If you're providing snippets in the issue, use Markdown code blocks . Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior. Explain which behavior you expected to see instead and why. If the problem wasn't triggered by a specific action , describe what you were doing before the problem happened and share more information using the guidelines below. Provide more context by answering these questions: Did the problem start happening recently (e.g. after updating to a new version) or was this always a problem? If the problem started happening recently, can you reproduce the problem in an older version? What's the most recent version in which the problem doesn't happen? Can you reliably reproduce the issue? If not, provide details about how often the problem happens and under which conditions it normally happens. Include details about your configuration and environment: Which version of the project are you using? What's the name and version of the OS you're using ? Any other information that could be useful about you environment Suggesting Enhancements This section guides you through submitting an enhancement suggestion for this project, including completely new features and minor improvements to existing functionality. Following these guidelines helps maintainers and the community understand your suggestion and find related suggestions. Before creating enhancement suggestions, please check the list of enhancements suggestions in the issue tracker as you might find out that you don't need to create one. When you are creating an enhancement suggestion, please include as many details as possible. How Do I Submit A (Good) Enhancement Suggestion? Enhancement suggestions are tracked as GitHub issues . Create an issue on the project's repository and provide the following information: Use a clear and descriptive title for the issue to identify the suggestion. Provide a step-by-step description of the suggested enhancement in as many details as possible. Provide specific examples to demonstrate the steps . It's always better to get more information. You can include links to files or GitHub projects, copy/pasteable snippets or even print screens or animated GIFS. If you're providing snippets in the issue, use Markdown code blocks . Describe the current behavior and explain which behavior you expected to see instead and why. List some other similar projects where this enhancement exists. Specify which version of the project you're using. Specify the current environment you're using. if this is a useful information. Creating Pull Requests How Do I Submit A (Good) Pull Request? Be warned that the contributors agreement displayed on top of this document is applicable as soon as you create a pull request . Use a clear and descriptive title for the pull request to state the improvement you made to the code or the bug you solved. Provide a link to the related issue if the pull request is a follow up of an existing bug report or enhancement suggestion. Comment why this pull request represents an enhancement and give a rationale explaining why you did it that way and not another way. Use the same coding style than the one used in this project . Welcome suggestions from the maintainers to improve your pull request .","title":"Contributions"},{"location":"CONTRIBUTING/#contributing-to-this-project","text":"Contributors Agreement Introduction I don't want to read this whole thing I just have a question!!! How Can I Contribute? Reporting Bugs Suggesting Enhancements Creating Pull Requests","title":"Contributing to this project"},{"location":"CONTRIBUTING/#contributors-agreement","text":"By submitting patches to this project you agree to allow them to be redistributed under the project's license, according to the normal forms and usages of the open-source community.","title":"Contributors Agreement"},{"location":"CONTRIBUTING/#introduction","text":"First off, thank you for considering contributing to this project. It's people like you that make it such a great tool. Following these guidelines helps to communicate that you respect the time of the developers managing and developing this open source project. In return, they should reciprocate that respect in addressing your issue, assessing changes, and helping you finalize your pull requests. This is an open source project and we love to receive contributions from our community \u2014 you! There are many ways to contribute, from writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests or writing code which can be incorporated into the main project itself.","title":"Introduction"},{"location":"CONTRIBUTING/#i-dont-want-to-read-this-whole-thing-i-just-have-a-question","text":"We currently allow our users to use the issue tracker for support questions. But please be wary that maintaining an open source project can take a lot of time from the maintainers. If asking for a support question, state it clearly and take the time to explain your problem properly. Also, if your problem is not strictly related to this project we recommend you to use Stack Overlow instead.","title":"I don't want to read this whole thing I just have a question!!!"},{"location":"CONTRIBUTING/#how-can-i-contribute","text":"","title":"How Can I Contribute?"},{"location":"CONTRIBUTING/#reporting-bugs","text":"Before creating bug reports, please check the existing bug reports as you might find out that you don't need to create one. When you are creating a bug report, please include as many details as possible.","title":"Reporting Bugs"},{"location":"CONTRIBUTING/#how-do-i-submit-a-good-bug-report","text":"Bugs are tracked as GitHub issues . Create an issue on the project's repository and provide the following information. Explain the problem and include additional details to help maintainers reproduce the problem: Use a clear and descriptive title for the issue to identify the problem. Describe the exact steps which reproduce the problem in as many details as possible. For example, start by explaining how you used the project. When listing steps, don't just say what you did, but explain how you did it . Provide specific examples to demonstrate the steps . It's always better to get more information. You can include links to files or GitHub projects, copy/pasteable snippets or even print screens or animated GIFS. If you're providing snippets in the issue, use Markdown code blocks . Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior. Explain which behavior you expected to see instead and why. If the problem wasn't triggered by a specific action , describe what you were doing before the problem happened and share more information using the guidelines below. Provide more context by answering these questions: Did the problem start happening recently (e.g. after updating to a new version) or was this always a problem? If the problem started happening recently, can you reproduce the problem in an older version? What's the most recent version in which the problem doesn't happen? Can you reliably reproduce the issue? If not, provide details about how often the problem happens and under which conditions it normally happens. Include details about your configuration and environment: Which version of the project are you using? What's the name and version of the OS you're using ? Any other information that could be useful about you environment","title":"How Do I Submit A (Good) Bug Report?"},{"location":"CONTRIBUTING/#suggesting-enhancements","text":"This section guides you through submitting an enhancement suggestion for this project, including completely new features and minor improvements to existing functionality. Following these guidelines helps maintainers and the community understand your suggestion and find related suggestions. Before creating enhancement suggestions, please check the list of enhancements suggestions in the issue tracker as you might find out that you don't need to create one. When you are creating an enhancement suggestion, please include as many details as possible.","title":"Suggesting Enhancements"},{"location":"CONTRIBUTING/#how-do-i-submit-a-good-enhancement-suggestion","text":"Enhancement suggestions are tracked as GitHub issues . Create an issue on the project's repository and provide the following information: Use a clear and descriptive title for the issue to identify the suggestion. Provide a step-by-step description of the suggested enhancement in as many details as possible. Provide specific examples to demonstrate the steps . It's always better to get more information. You can include links to files or GitHub projects, copy/pasteable snippets or even print screens or animated GIFS. If you're providing snippets in the issue, use Markdown code blocks . Describe the current behavior and explain which behavior you expected to see instead and why. List some other similar projects where this enhancement exists. Specify which version of the project you're using. Specify the current environment you're using. if this is a useful information.","title":"How Do I Submit A (Good) Enhancement Suggestion?"},{"location":"CONTRIBUTING/#creating-pull-requests","text":"","title":"Creating Pull Requests"},{"location":"CONTRIBUTING/#how-do-i-submit-a-good-pull-request","text":"Be warned that the contributors agreement displayed on top of this document is applicable as soon as you create a pull request . Use a clear and descriptive title for the pull request to state the improvement you made to the code or the bug you solved. Provide a link to the related issue if the pull request is a follow up of an existing bug report or enhancement suggestion. Comment why this pull request represents an enhancement and give a rationale explaining why you did it that way and not another way. Use the same coding style than the one used in this project . Welcome suggestions from the maintainers to improve your pull request .","title":"How Do I Submit A (Good) Pull Request?"},{"location":"RELEASE_NOTES/","text":"","title":"Release notes"},{"location":"about/","text":"","title":"About"},{"location":"adr/","text":"Architecture Decision Records Table of Contents Caching layer above RDBMS Caching layer above RDBMS Status: Proposed Drivers: Rajat Singhal Approvers: Pramod Verma, Vivek Raghavan Contributers: Rajat Singhal, Soujyo Sen, Heera Ballabh, Umair Manzoor Date: 2021-03-18 Technical Story: [ticket/issue URL] Context and Problem Statement Currently, the application service directly talks with RDBMS to fetch the sentences shown while doing Contributions and to fetch contributions while doing the Validations. The sentences and contributions are fetched in batches to avoid hitting the databse frequently. When the application will scale, there may be some contention at RDBMS layer. To avoid that having a Cache above the RDBS layer is being considered Considered Options Decision Outcome Positive Consequences Negative Consequences Pros and Cons of the Options","title":"Architecture Decision Records"},{"location":"adr/#architecture-decision-records","text":"","title":"Architecture Decision Records"},{"location":"adr/#table-of-contents","text":"Caching layer above RDBMS","title":"Table of Contents"},{"location":"adr/#caching-layer-above-rdbms","text":"Status: Proposed Drivers: Rajat Singhal Approvers: Pramod Verma, Vivek Raghavan Contributers: Rajat Singhal, Soujyo Sen, Heera Ballabh, Umair Manzoor Date: 2021-03-18 Technical Story: [ticket/issue URL]","title":"Caching layer above RDBMS"},{"location":"adr/#context-and-problem-statement","text":"Currently, the application service directly talks with RDBMS to fetch the sentences shown while doing Contributions and to fetch contributions while doing the Validations. The sentences and contributions are fetched in batches to avoid hitting the databse frequently. When the application will scale, there may be some contention at RDBMS layer. To avoid that having a Cache above the RDBS layer is being considered","title":"Context and Problem Statement"},{"location":"adr/#considered-options","text":"","title":"Considered Options"},{"location":"adr/#decision-outcome","text":"","title":"Decision Outcome"},{"location":"adr/#positive-consequences","text":"","title":"Positive Consequences"},{"location":"adr/#negative-consequences","text":"","title":"Negative Consequences"},{"location":"adr/#pros-and-cons-of-the-options","text":"","title":"Pros and Cons of the Options"},{"location":"adr_template/","text":"[short title of solved problem and solution] Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by ADR-0005 ] Deciders: [list everyone involved in the decision] Date: [YYYY-MM-DD when the decision was last updated] Technical Story: [description | ticket/issue URL] Context and Problem Statement [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] Decision Drivers [driver 1, e.g., a force, facing concern, \u2026] [driver 2, e.g., a force, facing concern, \u2026] \u2026 Considered Options [option 1] [option 2] [option 3] \u2026 Decision Outcome Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. Positive Consequences [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] \u2026 Negative Consequences [e.g., compromising quality attribute, follow-up decisions required, \u2026] \u2026 Pros and Cons of the Options [option 1] [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 [option 2] [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 [option 3] [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 Links [Link type] [Link to ADR] \u2026","title":"[short title of solved problem and solution]"},{"location":"adr_template/#short-title-of-solved-problem-and-solution","text":"Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by ADR-0005 ] Deciders: [list everyone involved in the decision] Date: [YYYY-MM-DD when the decision was last updated] Technical Story: [description | ticket/issue URL]","title":"[short title of solved problem and solution]"},{"location":"adr_template/#context-and-problem-statement","text":"[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]","title":"Context and Problem Statement"},{"location":"adr_template/#decision-drivers","text":"[driver 1, e.g., a force, facing concern, \u2026] [driver 2, e.g., a force, facing concern, \u2026] \u2026","title":"Decision Drivers "},{"location":"adr_template/#considered-options","text":"[option 1] [option 2] [option 3] \u2026","title":"Considered Options"},{"location":"adr_template/#decision-outcome","text":"Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].","title":"Decision Outcome"},{"location":"adr_template/#positive-consequences","text":"[e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] \u2026","title":"Positive Consequences "},{"location":"adr_template/#negative-consequences","text":"[e.g., compromising quality attribute, follow-up decisions required, \u2026] \u2026","title":"Negative Consequences "},{"location":"adr_template/#pros-and-cons-of-the-options","text":"","title":"Pros and Cons of the Options "},{"location":"adr_template/#option-1","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 1]"},{"location":"adr_template/#option-2","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 2]"},{"location":"adr_template/#option-3","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 3]"},{"location":"adr_template/#links","text":"[Link type] [Link to ADR] \u2026","title":"Links "},{"location":"crowdsource_platform/","text":"Crowdsourcing Platform Table of Contents Crowdsourcing Platform Table of Contents About The Project Built With Architecture Languages and Tools Dashboard Design CI/CD Infrastructure as Code Getting Started Prerequisites Installation Usage Common configuration steps: Setting credentials for Google cloud bucket Setting credentials for AWS cloud bucket Bucket configuration Environment file configurations Running services Database migrations Testing Unit Tests Functional Test Scalabiity Test Load Test Security Running cost estimates Architecture Decision Records Contributing License Contact About The Project This is a web application which can be used to crowdsource audio and validate them for various languages. The application makes use of NodeJs, Postgres for Database. It can be hosted on any cloud platform. The current application has code to support AWS and GCP as providers to store the recorded information. Crowdsourcing Platform\u2019s developer documentation is meant for its adopters, developers and contributors. The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Crowdsourcing Platform. Data Collection Pipeline is based on an open platform, you are free to use any programming language to extend or customize it but we prefer to use python to perform smart scraping. The Developer documentation provides you with a complete set of guidelines which you need to: Install dependencies for the Crowdsourcing Platform Configure Crowdsourcing Platform Customize Crowdsourcing Platform Extend Crowdsourcing Platform Contribute to Crowdsourcing Platform Built With We have used Node.js to build this platform. * Node Architecture Languages and Tools Dashboard Design The transactional tables and view tables are kept separate. Materialized views are used which holds the data as well. This avoids on the fly computations for aggregation for each query. The materizaled view are refreshed every 4 hours As a part of the refresh job, the aggregated data is dumped as json that is be served directly via CDN. Advantages: Faster reads: Separate view with only 365 aggregated data points per year. Less overhead on DB as data queried is on a very small data set and served from S3 buckets Transactional tables are optimized for faster writes as we have separate views for reads Simplified read queries as complexity is abstracted in views AWS RDS managed DB. Can be scaled horizontally and vertically easily if required in future. CI/CD CircleCI is used for CI/CD. Unit tests are run continously for each commit Functional Tests are run continously for each commit and act as one if the quality gates before Production deployment Automated deployment to K8s for multiple environments Database schema changes are done continously and automatically Trunk based developement is followed Infrastructure as Code Infrastructure defined in code with Terraform and shell scripts Easily migrate to another AWS account Spin up new env easily Getting Started To get started install the prerequisites and clone the repo to machine on which you wish to run the application. Prerequisites Install node library using commands mentioned below. For any linux based operating system (preferred Ubuntu): sudo apt-get install nodejs For Mac-os: brew install node Windows user can follow installation steps on https://nodejs.org/en/#home-downloadhead Install or connect to a postgres database Get credentials from google developer console for google cloud storage access/ or aws cli for amazon s3 storage access. Installation Clone the repo using git clone https://github.com/Open-Speech-EkStep/crowdsource-dataplatform.git Go inside the directory cd crowdsource-dataplatform Install node requirements npm install Usage Common configuration steps: Setting credentials for Google cloud bucket You can set credentials for Google cloud bucket by running the following command gcloud auth application-default login Setting credentials for AWS cloud bucket You can set credentials for AWS cloud bucket by running the following command aws configure Bucket configuration You can create a specific bucket to store the recorded samples on aws or gcp. And mention those in the environment variables. Environment file configurations The following are the variables required to run the application, for running on local these can be added to a .env file DB_HOST: The host url where your postgres instance is running DB_USER: The username to access the db DB_NAME: The database name DEV_DB_NAME: The database name specific to dev environment DB_PASS: The database password BUCKET_NAME: The bucket name configured on aws or gcp ENCRYPTION_KEY: Key to run unit tests PORT: Port to run the application on Running services Make sure the google credentials are present in project root folder in credentials.json file. You can run the project using the command npm run To run application using a Google cloud bucket npm run gcp To run application using a AWS cloud bucket npm run aws Database migrations This package is used to do migrations. To create the current database structure in your postgres instance, run the following command: db-migrate up It would read the configurations from the path migations/config/migration_config.json Once can also run the migrate up command by setting an environment variable DATABASE_URL=postgresql://${DB_USER}:${DB_PASS}@${DB_HOST}/${DB_NAME} To add a new migration db-migrate create add-new-table Using the above command with the --sqlFile flag would create corresponding .sql files in which one can write sql commands to do the operation. To rollback the last migration, one can db-migrate down Documentation for the package can be found here Testing Multiple types of tests are continously performed to make sure the application is in healthy state. Pyramid approach is followed with Unit tests at the base and Exploratory tests on top. Unit Tests Unit tests can be run using below command npm test Functional Test Functional tests can be run using below command npm run functional_test -- --env (test|dev) Scalabiity Test Scalabiity tests performed to verify that the system is elastically scalable Below tests were performed Test Objective: Scalability Test - Validate elastic scalability Resource Configuration: Environment: Dev Pod resources: 0.25 CPU/ 250M RAM Horizontal Pod Autoscaler : Scaling Threshold - 10% CPU Utilization Min pods: 1 Max Pods: 10 Test configuration: Number of concurrent users: 1000 Total Requests : 15000 Expected: Pods should scale if load increases and CPU utilization goes beyond 10% and should scale down after 5 mins Actual : Pods were scaled up after the CPU utilization went past 10%. Time to scale to desired state was around 2-3 mins Outcome: PASSED As surge started, pods started spinning up Load Test Load testing is performed to verify the system is able to handle 5K concurrent users without much impact on latency Test Objective: Load Test - Validate if application can handle 5K concurrent users Resource Configuration: Environment: Test Initial Pods: 3 Pod resources: 2 CPU/ 2GB RAM Horizontal Pod Autoscaler : Scaling Threshold - 40% CPU Utilization Min pods: 3 , Max Pods: 10 Test configuration: Number of concurrent users: 20000 Requests per user : 3 Ramp up time: 10 sec Iterations: 3 Outcome: PASSED ELB stats: Database stats: Summary: - This test had 20000 users ramped up within 1 min (3 times). - The test was performed from a single machine so 20K concurrent users could scale in 1 min. - All the requests were served within initial resources, no scaling was triggered. - All three endpoints served response in around 2 sec on an average. - The system was able to handle upto 12K concurrent users. - There were some errors thrown by AWS Load balancer may be due to single IP requests. - Database could handle the load and no connection leak is observed Security Security first approach is taken while building this application. The OWASP top 10 are ingrained in the application security DNA. Please reach out to srajat@thoughtworks or heerabal@thoughtworks.com for more information around Security Running cost estimates Cloud : AWS Amazon RDS (4 CPU): $400 WAF: $30 EKS + Fargate: $75 + $225 = $300 ELB: $150 Others: $200 Total: ~ $1100-1200 per month Architecture Decision Records Decision records are maintained HERE Cache above RDBMS Contributing Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request License Distributed under the [MIT] License. See LICENSE for more information. Contact Project Link: https://github.com/Open-Speech-EkStep/crowdsource-dataplatform/","title":"Crowdsourcing Platform"},{"location":"crowdsource_platform/#crowdsourcing-platform","text":"","title":"Crowdsourcing Platform"},{"location":"crowdsource_platform/#table-of-contents","text":"Crowdsourcing Platform Table of Contents About The Project Built With Architecture Languages and Tools Dashboard Design CI/CD Infrastructure as Code Getting Started Prerequisites Installation Usage Common configuration steps: Setting credentials for Google cloud bucket Setting credentials for AWS cloud bucket Bucket configuration Environment file configurations Running services Database migrations Testing Unit Tests Functional Test Scalabiity Test Load Test Security Running cost estimates Architecture Decision Records Contributing License Contact","title":"Table of Contents"},{"location":"crowdsource_platform/#about-the-project","text":"This is a web application which can be used to crowdsource audio and validate them for various languages. The application makes use of NodeJs, Postgres for Database. It can be hosted on any cloud platform. The current application has code to support AWS and GCP as providers to store the recorded information. Crowdsourcing Platform\u2019s developer documentation is meant for its adopters, developers and contributors. The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Crowdsourcing Platform. Data Collection Pipeline is based on an open platform, you are free to use any programming language to extend or customize it but we prefer to use python to perform smart scraping. The Developer documentation provides you with a complete set of guidelines which you need to: Install dependencies for the Crowdsourcing Platform Configure Crowdsourcing Platform Customize Crowdsourcing Platform Extend Crowdsourcing Platform Contribute to Crowdsourcing Platform","title":"About The Project"},{"location":"crowdsource_platform/#built-with","text":"We have used Node.js to build this platform. * Node","title":"Built With"},{"location":"crowdsource_platform/#architecture","text":"","title":"Architecture"},{"location":"crowdsource_platform/#languages-and-tools","text":"","title":"Languages and Tools"},{"location":"crowdsource_platform/#dashboard-design","text":"The transactional tables and view tables are kept separate. Materialized views are used which holds the data as well. This avoids on the fly computations for aggregation for each query. The materizaled view are refreshed every 4 hours As a part of the refresh job, the aggregated data is dumped as json that is be served directly via CDN. Advantages: Faster reads: Separate view with only 365 aggregated data points per year. Less overhead on DB as data queried is on a very small data set and served from S3 buckets Transactional tables are optimized for faster writes as we have separate views for reads Simplified read queries as complexity is abstracted in views AWS RDS managed DB. Can be scaled horizontally and vertically easily if required in future.","title":"Dashboard Design"},{"location":"crowdsource_platform/#cicd","text":"CircleCI is used for CI/CD. Unit tests are run continously for each commit Functional Tests are run continously for each commit and act as one if the quality gates before Production deployment Automated deployment to K8s for multiple environments Database schema changes are done continously and automatically Trunk based developement is followed","title":"CI/CD"},{"location":"crowdsource_platform/#infrastructure-as-code","text":"Infrastructure defined in code with Terraform and shell scripts Easily migrate to another AWS account Spin up new env easily","title":"Infrastructure as Code"},{"location":"crowdsource_platform/#getting-started","text":"To get started install the prerequisites and clone the repo to machine on which you wish to run the application.","title":"Getting Started"},{"location":"crowdsource_platform/#prerequisites","text":"Install node library using commands mentioned below. For any linux based operating system (preferred Ubuntu): sudo apt-get install nodejs For Mac-os: brew install node Windows user can follow installation steps on https://nodejs.org/en/#home-downloadhead Install or connect to a postgres database Get credentials from google developer console for google cloud storage access/ or aws cli for amazon s3 storage access.","title":"Prerequisites"},{"location":"crowdsource_platform/#installation","text":"Clone the repo using git clone https://github.com/Open-Speech-EkStep/crowdsource-dataplatform.git Go inside the directory cd crowdsource-dataplatform Install node requirements npm install","title":"Installation"},{"location":"crowdsource_platform/#usage","text":"","title":"Usage"},{"location":"crowdsource_platform/#common-configuration-steps","text":"","title":"Common configuration steps:"},{"location":"crowdsource_platform/#setting-credentials-for-google-cloud-bucket","text":"You can set credentials for Google cloud bucket by running the following command gcloud auth application-default login","title":"Setting credentials for Google cloud bucket"},{"location":"crowdsource_platform/#setting-credentials-for-aws-cloud-bucket","text":"You can set credentials for AWS cloud bucket by running the following command aws configure","title":"Setting credentials for AWS cloud bucket"},{"location":"crowdsource_platform/#bucket-configuration","text":"You can create a specific bucket to store the recorded samples on aws or gcp. And mention those in the environment variables.","title":"Bucket configuration"},{"location":"crowdsource_platform/#environment-file-configurations","text":"The following are the variables required to run the application, for running on local these can be added to a .env file DB_HOST: The host url where your postgres instance is running DB_USER: The username to access the db DB_NAME: The database name DEV_DB_NAME: The database name specific to dev environment DB_PASS: The database password BUCKET_NAME: The bucket name configured on aws or gcp ENCRYPTION_KEY: Key to run unit tests PORT: Port to run the application on","title":"Environment file configurations"},{"location":"crowdsource_platform/#running-services","text":"Make sure the google credentials are present in project root folder in credentials.json file. You can run the project using the command npm run To run application using a Google cloud bucket npm run gcp To run application using a AWS cloud bucket npm run aws","title":"Running services"},{"location":"crowdsource_platform/#database-migrations","text":"This package is used to do migrations. To create the current database structure in your postgres instance, run the following command: db-migrate up It would read the configurations from the path migations/config/migration_config.json Once can also run the migrate up command by setting an environment variable DATABASE_URL=postgresql://${DB_USER}:${DB_PASS}@${DB_HOST}/${DB_NAME} To add a new migration db-migrate create add-new-table Using the above command with the --sqlFile flag would create corresponding .sql files in which one can write sql commands to do the operation. To rollback the last migration, one can db-migrate down Documentation for the package can be found here","title":"Database migrations"},{"location":"crowdsource_platform/#testing","text":"Multiple types of tests are continously performed to make sure the application is in healthy state. Pyramid approach is followed with Unit tests at the base and Exploratory tests on top.","title":"Testing"},{"location":"crowdsource_platform/#unit-tests","text":"Unit tests can be run using below command npm test","title":"Unit Tests"},{"location":"crowdsource_platform/#functional-test","text":"Functional tests can be run using below command npm run functional_test -- --env (test|dev)","title":"Functional Test"},{"location":"crowdsource_platform/#scalabiity-test","text":"Scalabiity tests performed to verify that the system is elastically scalable Below tests were performed Test Objective: Scalability Test - Validate elastic scalability Resource Configuration: Environment: Dev Pod resources: 0.25 CPU/ 250M RAM Horizontal Pod Autoscaler : Scaling Threshold - 10% CPU Utilization Min pods: 1 Max Pods: 10 Test configuration: Number of concurrent users: 1000 Total Requests : 15000 Expected: Pods should scale if load increases and CPU utilization goes beyond 10% and should scale down after 5 mins Actual : Pods were scaled up after the CPU utilization went past 10%. Time to scale to desired state was around 2-3 mins Outcome: PASSED As surge started, pods started spinning up","title":"Scalabiity Test"},{"location":"crowdsource_platform/#load-test","text":"Load testing is performed to verify the system is able to handle 5K concurrent users without much impact on latency Test Objective: Load Test - Validate if application can handle 5K concurrent users Resource Configuration: Environment: Test Initial Pods: 3 Pod resources: 2 CPU/ 2GB RAM Horizontal Pod Autoscaler : Scaling Threshold - 40% CPU Utilization Min pods: 3 , Max Pods: 10 Test configuration: Number of concurrent users: 20000 Requests per user : 3 Ramp up time: 10 sec Iterations: 3 Outcome: PASSED ELB stats: Database stats: Summary: - This test had 20000 users ramped up within 1 min (3 times). - The test was performed from a single machine so 20K concurrent users could scale in 1 min. - All the requests were served within initial resources, no scaling was triggered. - All three endpoints served response in around 2 sec on an average. - The system was able to handle upto 12K concurrent users. - There were some errors thrown by AWS Load balancer may be due to single IP requests. - Database could handle the load and no connection leak is observed","title":"Load Test"},{"location":"crowdsource_platform/#security","text":"Security first approach is taken while building this application. The OWASP top 10 are ingrained in the application security DNA. Please reach out to srajat@thoughtworks or heerabal@thoughtworks.com for more information around Security","title":"Security"},{"location":"crowdsource_platform/#running-cost-estimates","text":"Cloud : AWS Amazon RDS (4 CPU): $400 WAF: $30 EKS + Fargate: $75 + $225 = $300 ELB: $150 Others: $200 Total: ~ $1100-1200 per month","title":"Running cost estimates"},{"location":"crowdsource_platform/#architecture-decision-records","text":"Decision records are maintained HERE Cache above RDBMS","title":"Architecture Decision Records"},{"location":"crowdsource_platform/#contributing","text":"Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request","title":"Contributing"},{"location":"crowdsource_platform/#license","text":"Distributed under the [MIT] License. See LICENSE for more information.","title":"License"},{"location":"crowdsource_platform/#contact","text":"Project Link: https://github.com/Open-Speech-EkStep/crowdsource-dataplatform/","title":"Contact"},{"location":"data_collection/","text":"Data Collection Pipeline Table of Contents Data Collection Pipeline Table of Contents About The Project Built With Summary Getting Started Prerequisites Installation Usage Common configuration steps: Setting credentials for Google cloud bucket Bucket configuration Metadata file configurations Youtube download configurations Youtube API configuration Web Crawl Configuration Adding new spider Running services Youtube spider in channel mode: Youtube spider in file mode: Bing Spider Urls Spider Selenium google crawler Selenium youtube crawler for file mode and api mode Contributing License Contact Acknowledgements About The Project This is downloading framework that is extensible and allows the user to add new source without much code changes. For each new source user need to write a scrapy spider script and rest of downloading and meta file creation is handled by repective pipelines. And if required user can add their custom pipelines. This framework automatically transfer the downloaded data to a Google cloud bucket automatically. For more info on writing scrapy spider and pipeline one can refer to the documentation . Data Collection Pipeline\u2019s developer documentation is meant for its adopters, developers and contributors. The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Data Collection Pipeline. Data Collection Pipeline is based on an open platform, you are free to use any programming language to extend or customize it but we prefer to use python to perform smart scraping. The Developer documentation provides you with a complete set of guidelines which you need to: Install Data Collection Pipeline Configure Data Collection Pipeline Customize Data Collection Pipeline Extend Data Collection Pipeline Contribute to Data Collection Pipeline Built With We have used scrapy as the base of this framework. * Scrapy Summary This summary mentions the key advantages and limitations of this smart crawler service. Youtube Crawler Key Points and Advantages: Get language relevant channels from YouTube and download videos from them.(70%-80% relevancy with language - based on Manual Analysis) Can fetch channels with Creative Commons video and download the videos in them as well.(70% relevancy with language) Can download using file mode(manually filled with video Ids) or channel mode. Youtube-dl can fetch N number of videos from a channel and download them. YouTube crawler downloads files at a rate of maximum of 2000 hours per day and minimum of 800 hours per day. Youtube crawler is more convenient and it\u2019s a main source of Creative Commons data that can be accessed easily. It can be deployed in cloud service called zyte used for scraping/crawling. License information of videos are available in metadata. Limitations: Youtube-api cannot return more than 500 videos per channel.(when using YOUTUBE_API mode in configuration) Youtube-api is restricted to 10000 tokens per day in free mode. 10000 tokens can be used to get license info of 10000 videos.(in any mode) 10000 tokens can be used to get 5000 channels.(in YOUTUBE_API mode) Youtube-dl can be used to get all videos freely.(in YOUTUBE_DL mode) Cannot fetch data from specific playlist. (Solution: Fetch videos Ids of a playlist using YouTube-dl and put them in a file and download in file mode.) Rare cases in which you might get Too many requests error from Youtube-DL. (Solution: Rerun the application with same sources.) Cannot download videos which require user information and private videos. Web Crawler Key Points and Advantages: Web crawler can download specific language audio but with around 50 - 60% relevance. Web crawler downloads files at a rate of at least 2000 hours per day. It is a faster means of downloading data. Creative Commons license of videos can be identified if available while crawling websites. Limitations: Web crawler is not finely tuned yet, so downloaded content might have low language relevance. It cannot be deployed in zyte service free accounts and can be only deployed in zyte service paid accounts where docker container creation can be customised. License information of videos in web crawler cannot be automatically identified but requires some manual intervention. Getting Started To get started install the prerequisites and clone the repo to machine on which you wish to run the framework. Prerequisites Install ffmpeg library using commands mentioned below. For any linux based operating system (preferred Ubuntu): sudo apt-get install ffmpeg For Mac-os: brew install ffmpeg Windows user can follow installation steps on https://www.ffmpeg.org Install Python Version = 3.6 Get credentials from google developer console for google cloud storage access. Installation Clone the repo using git clone https://github.com/Open-Speech-EkStep/data-acquisition-pipeline.git Go inside the directory cd data-acquisition-pipeline Install python requirements pip install -r requirements.txt Usage This framework allows the user to download the media file from a websource(youtube, xyz.com, etc) and creates the respective metadata file from the data that is extracted from the file.For using any added source or to add new source refer to steps below.It can also crawl internet for media of a specific language. For web crawling, refer to the web crawl configuration below. Common configuration steps: Setting credentials for Google cloud bucket You can set credentials for Google cloud bucket in the credentials.json add the credentials in given manner It can be found in the project root folder. {\"Credentials\":{ YOUR ACCOUNT CREDENTIAL KEYS }} Note: All configuration files can be found in the following path data-acquisition-pipeline/data_acquisition_framework/configs/ Bucket configuration Bucket configurations for data transfer in storage_config.json \"bucket\": \"ekstepspeechrecognition-dev\", Your bucket name \"channel_blob_path\": \"scrapydump/refactor_test\", Path to directory where downloaded files is to be stored \"archive_blob_path\": \"archive\", Folder name in which history of download is to be maintained \"channels_file_blob_path\": \"channels\", Folder name in which channels and its videos are saved \"scraped_data_blob_path\": \"data_to_be_scraped\" Folder name in which CSV for youtube file mode is stored Note: 1. The scraped_data_blob_path folder should be present inside the channel_blob_path folder. 2. The CSV file used in file mode of youtube and its name must be same as source_name given above. 3. (only for datacollector_urls and datacollector_bing spiders) To autoconfigure language parameter to channel_blob_path from web_crawler_config.json, use <language> in channel_blob_path. \"eg: for tamil : data/download/<language>/audio - this will replace <language> with tamil.\" 4. The archive_blob_path and channels_file_blob_path are folders that will be autogenerated in bucket with the given name. Metadata file configurations Metadata file configurations in config.json mode: 'complete' This should not be changed audio_id: null If you want to give a custom audio id add here cleaned_duration: null If you know the cleaned duration of audio add here num_of_speakers: null Number of speaker present in audio language: Hindi Language of audio has_other_audio_signature: False If audio has multiple speaker in same file (True/False) type: 'audio' Type of media (audio or video) source: 'Demo_Source' Source name experiment_use: False If its for experimental use (True/False) utterances_files_list: null source_website: '' Source website url experiment_name: null Name of experiment if experiment_use is True mother_tongue: null Accent of language(Bengali, Marathi, etc...) age_group: null Age group of speaker in audio recorded_state: null State in which audio is recorded recorded_district: null District of state in which audio is recorded recorded_place: null Recording location recorded_date: null Recording date purpose: null Purpose of recording speaker_gender: null Gender of speaker speaker_name: null Name of speaker Note: 1. If any of the field info is not available keep its value to null 2. If speaker_name or speaker_gender is given then that same will be used for all the files in given source Youtube download configurations You can set download mode [file/channel] in youtube_pipeline_config.py mode = 'file' # [channel,file] In file mode you will store a csv file whose name must be same as source name in scraped_data_blob_path. csv must contain urls of youtube videos, speaker name and gender as three different columns. Urls is a must field. You can leave speaker name and gender blank if data is not available. Given below is the structure of csv. video_url,speaker_name,speaker_gender https://www.youtube.com/watch?v=K1vW_ZikA5o,Ram_Singh,male https://www.youtube.com/watch?v=o82HIOgozi8,John_Doe,male ... Common configurations in youtube_pipeline_config.py # Common configurations \"source_name\": \"DEMO\", This is the name of source you are downloading batch_num = 1 Number of videos to be downloaded as batches youtube_service_to_use = YoutubeService.YOUTUBE_DL This field is to choose which service to use for getting video information only_creative_commons = False Should Download only creative commons(True, False) Possible values for youtube_service_to_use: (YoutubeService.YOUTUBE_DL, YoutubeService.YOUTUBE_API) * File mode configurations in youtube_pipeline_config.py # File Mode configurations file_speaker_gender_column = 'speaker_gender' Gender column name in csv file file_speaker_name_column = \"speaker_name\" Speaker name column name in csv file file_url_name_column = \"video_url\" Video url column name in csv file license_column = \"license\" Video license column name in csv file channel mode configuration in youtube_pipeline_config.py # Channel mode configurations channel_url_dict = {} Channel url dictionary (This will download all the videos from the given channels with corresponding source names) Note: 1. In channel_url_dict, the keys must be the urls and values must be their channel names 2. To get list of channels from youtube API, channel_url_dict must be empty Youtube API configuration Automated Youtube fetching configuration in youtube_api_config.json # Youtube API configurations \"language\" : \"hindi\", Type of language for which search results are required. \"language_code\": \"hi\", Language code for the specified language. \"keywords\":[ The search keywords to be given in youtube API query \"audio\", \"speech\", \"talk\" ], \"words_to_ignore\":[ The words that are to be ignored in youtube API query \"song\", \"music\" ], \"max_results\": 20 Maximum number of channels or results that is required. Web Crawl Configuration web crawl configuration in web_crawl_config.json (Use this only for datacollector_bing and datacollector_urls spider) \"language\": \"gujarati\", Language to be crawled \"language_code\": \"gu\", Language code for the specified language. \"keywords\": [ Keywords to query \"talks audio\", \"audiobooks\", \"speeches\", ], \"word_to_ignore\": [ Words to ignore while crawling \"ieeexplore.ieee.org\", \"dl.acm.org\", \"www.microsoft.com\" ], \"extensions_to_ignore\": [ Formats/extensions to ignore while crawling \".jpeg\", \"xlsx\", \".xml\" ], \"extensions_to_include\": [ Formats/extensions to include while crawling \".mp3\", \".wav\", \".mp4\", ], \"pages\": 1, Number of pages to crawl \"depth\": 1, Nesting depth for each website \"continue_page\": \"NO\", Field to continue/resume crawling \"last_visited\": 200, Last visited results count \"enable_hours_restriction\": \"YES\", Restrict crawling based on hours of data collected \"max_hours\": 1 Maximum hours to crawl Adding new spider As we already mentioned our framework is extensible for any new source. To add a new source user just need to write a spider for that source. To add a spider you can follow the scrapy documentation or you can check our sample spider. Running services Make sure the google credentials are present in project root folder in credentials.json file. Youtube spider in channel mode: In data_acqusition_framework/configs , do the following: Open config.json and change language and type to your respective use case. Open storage_config.json and change bucket and channel_blob_path to your respective gcp paths.(For more info on these fields, scroll above to Bucket configuration) Open youtube_pipeline_config.py and change mode to channel (eg: mode='channel') There are two ways to download videos of youtube channels: You can hardcode the channel url and channel name. You can use youtube-utils service(youtube-dl/youtube data api) to fetch channels and its respective videos information. To download by hardcoding the channel urls, do the following: Open data_acqusition_framework/configs/youtube_pipeline_config.py and do the following: Add the channel_urls and its names in channel_url_dict variable. eg. channel_url_dict = { \"https://www.youtube.com/channel/UC2XEzs5R1mn2wTKgtjuMxiQ\": \"channel_name_a\", \"https://www.youtube.com/channel/UC2XEzs5R1mn2wTKgtjuMxiQ\":\"channel_name_b\" } Set youtube_service_to_use variable value to either YoutubeService.YOUTUBE_DL or YoutubeService.YOUTUBE_API for collecting video info. If YoutubeService.YOUTUBE_API is chosen, then get APIKEY for youtube data api from google developer console and store it in a file called .youtube_api_key in project root folder. From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos from youtube for the given channels and download them to bucket. To download by using youtube-utils service, do the following: Open data_acqusition_framework/configs/youtube_pipeline_config.py and do the following: Assign channel_url_dict = {} (If not empty, will not work). Set youtube_service_to_use variable value to either YoutubeService.YOUTUBE_DL or YoutubeService.YOUTUBE_API for collecting video info. If YoutubeService.YOUTUBE_API is chosen, then get APIKEY for youtube data api from google developer console and store it in a file called .youtube_api_key in project root folder. Open data_acqusition_framework/configs/youtube_api_config.json and change the fields to your requirements.(For more info: check above in Youtube api configuration) From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos from youtube for the given channels and download them to bucket. Youtube spider in file mode: In data_acqusition_framework/configs , do the following: Open config.json and change language and type to your respective use case. Open storage_config.json and change bucket and channel_blob_path to your respective gcp paths.(For more info on these fields, scroll above to Bucket configuration) Open youtube_pipeline_config.py and do the following: change mode to file (eg: mode='file'). change source_name to your requirement so that videos get downloaded to that folder in google storage bucket. Next Steps: Create a file in the following format: eg. source_name.csv with content (license column is optional): Here source_name in source_name.csv is the name you gave in youtube_pipeline_config.py file. It should be the same. video_url,speaker_name,speaker_gender,license https://www.youtube.com/watch?v=K1vW_ZikA5o,Ram_Singh,male,Creative Commons https://www.youtube.com/watch?v=o82HIOgozi8,John_Doe,male,Standard Youtube ... Now to upload this file to google cloud storage do the following: Open the channel_blob_path folder that you gave in storage_config.json and create a folder there named data_to_be_scraped . Upload the file that you created with previous step to this folder. From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos mentioned in the file from youtube and download them to bucket. Bing Spider Configure data_acquisition_framework/configs/web_crawl_config.json for your requirements. Starting datacollector_bing spider with audio pipeline. From project root folder, run the following: scrapy crawl datacollector_bing Urls Spider Configure data_acquisition_framework/configs/web_crawl_config.json for your requirements. Starting datacollector_urls spider with audio pipeline. Make sure to put the urls to crawl in the data_acquisition_framework/urls.txt . From project root folder, run the following: scrapy crawl datacollector_urls Selenium google crawler It is capable of crawling search results of google for a given language and exporting them to urls.txt file. This urls.txt file can be used with datacollector_urls spider to crawl all the search results website and download the media along with their metadata. A specified Readme can be found in selenium_google_crawler folder. Readme for selenium google crawler Selenium youtube crawler for file mode and api mode It is capable of crawling youtube videos using youtube api or from a list of files with youtube video ids provided with channel name as filename. A specified Readme can be found in selenium_youtube_crawler folder. Readme for selenium youtube crawler Contributing Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request License Distributed under the [XYZ] License. See LICENSE for more information. Contact Your Name - @your_twitter - email@example.com Project Link: https://github.com/your_username/repo_name Acknowledgements Scrapy YouTube-dl TinyTag","title":"Data Collection Pipeine"},{"location":"data_collection/#data-collection-pipeline","text":"","title":"Data Collection Pipeline"},{"location":"data_collection/#table-of-contents","text":"Data Collection Pipeline Table of Contents About The Project Built With Summary Getting Started Prerequisites Installation Usage Common configuration steps: Setting credentials for Google cloud bucket Bucket configuration Metadata file configurations Youtube download configurations Youtube API configuration Web Crawl Configuration Adding new spider Running services Youtube spider in channel mode: Youtube spider in file mode: Bing Spider Urls Spider Selenium google crawler Selenium youtube crawler for file mode and api mode Contributing License Contact Acknowledgements","title":"Table of Contents"},{"location":"data_collection/#about-the-project","text":"This is downloading framework that is extensible and allows the user to add new source without much code changes. For each new source user need to write a scrapy spider script and rest of downloading and meta file creation is handled by repective pipelines. And if required user can add their custom pipelines. This framework automatically transfer the downloaded data to a Google cloud bucket automatically. For more info on writing scrapy spider and pipeline one can refer to the documentation . Data Collection Pipeline\u2019s developer documentation is meant for its adopters, developers and contributors. The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Data Collection Pipeline. Data Collection Pipeline is based on an open platform, you are free to use any programming language to extend or customize it but we prefer to use python to perform smart scraping. The Developer documentation provides you with a complete set of guidelines which you need to: Install Data Collection Pipeline Configure Data Collection Pipeline Customize Data Collection Pipeline Extend Data Collection Pipeline Contribute to Data Collection Pipeline","title":"About The Project"},{"location":"data_collection/#built-with","text":"We have used scrapy as the base of this framework. * Scrapy","title":"Built With"},{"location":"data_collection/#summary","text":"This summary mentions the key advantages and limitations of this smart crawler service. Youtube Crawler Key Points and Advantages: Get language relevant channels from YouTube and download videos from them.(70%-80% relevancy with language - based on Manual Analysis) Can fetch channels with Creative Commons video and download the videos in them as well.(70% relevancy with language) Can download using file mode(manually filled with video Ids) or channel mode. Youtube-dl can fetch N number of videos from a channel and download them. YouTube crawler downloads files at a rate of maximum of 2000 hours per day and minimum of 800 hours per day. Youtube crawler is more convenient and it\u2019s a main source of Creative Commons data that can be accessed easily. It can be deployed in cloud service called zyte used for scraping/crawling. License information of videos are available in metadata. Limitations: Youtube-api cannot return more than 500 videos per channel.(when using YOUTUBE_API mode in configuration) Youtube-api is restricted to 10000 tokens per day in free mode. 10000 tokens can be used to get license info of 10000 videos.(in any mode) 10000 tokens can be used to get 5000 channels.(in YOUTUBE_API mode) Youtube-dl can be used to get all videos freely.(in YOUTUBE_DL mode) Cannot fetch data from specific playlist. (Solution: Fetch videos Ids of a playlist using YouTube-dl and put them in a file and download in file mode.) Rare cases in which you might get Too many requests error from Youtube-DL. (Solution: Rerun the application with same sources.) Cannot download videos which require user information and private videos. Web Crawler Key Points and Advantages: Web crawler can download specific language audio but with around 50 - 60% relevance. Web crawler downloads files at a rate of at least 2000 hours per day. It is a faster means of downloading data. Creative Commons license of videos can be identified if available while crawling websites. Limitations: Web crawler is not finely tuned yet, so downloaded content might have low language relevance. It cannot be deployed in zyte service free accounts and can be only deployed in zyte service paid accounts where docker container creation can be customised. License information of videos in web crawler cannot be automatically identified but requires some manual intervention.","title":"Summary"},{"location":"data_collection/#getting-started","text":"To get started install the prerequisites and clone the repo to machine on which you wish to run the framework.","title":"Getting Started"},{"location":"data_collection/#prerequisites","text":"Install ffmpeg library using commands mentioned below. For any linux based operating system (preferred Ubuntu): sudo apt-get install ffmpeg For Mac-os: brew install ffmpeg Windows user can follow installation steps on https://www.ffmpeg.org Install Python Version = 3.6 Get credentials from google developer console for google cloud storage access.","title":"Prerequisites"},{"location":"data_collection/#installation","text":"Clone the repo using git clone https://github.com/Open-Speech-EkStep/data-acquisition-pipeline.git Go inside the directory cd data-acquisition-pipeline Install python requirements pip install -r requirements.txt","title":"Installation"},{"location":"data_collection/#usage","text":"This framework allows the user to download the media file from a websource(youtube, xyz.com, etc) and creates the respective metadata file from the data that is extracted from the file.For using any added source or to add new source refer to steps below.It can also crawl internet for media of a specific language. For web crawling, refer to the web crawl configuration below.","title":"Usage"},{"location":"data_collection/#common-configuration-steps","text":"","title":"Common configuration steps:"},{"location":"data_collection/#setting-credentials-for-google-cloud-bucket","text":"You can set credentials for Google cloud bucket in the credentials.json add the credentials in given manner It can be found in the project root folder. {\"Credentials\":{ YOUR ACCOUNT CREDENTIAL KEYS }} Note: All configuration files can be found in the following path data-acquisition-pipeline/data_acquisition_framework/configs/","title":"Setting credentials for Google cloud bucket"},{"location":"data_collection/#bucket-configuration","text":"Bucket configurations for data transfer in storage_config.json \"bucket\": \"ekstepspeechrecognition-dev\", Your bucket name \"channel_blob_path\": \"scrapydump/refactor_test\", Path to directory where downloaded files is to be stored \"archive_blob_path\": \"archive\", Folder name in which history of download is to be maintained \"channels_file_blob_path\": \"channels\", Folder name in which channels and its videos are saved \"scraped_data_blob_path\": \"data_to_be_scraped\" Folder name in which CSV for youtube file mode is stored Note: 1. The scraped_data_blob_path folder should be present inside the channel_blob_path folder. 2. The CSV file used in file mode of youtube and its name must be same as source_name given above. 3. (only for datacollector_urls and datacollector_bing spiders) To autoconfigure language parameter to channel_blob_path from web_crawler_config.json, use <language> in channel_blob_path. \"eg: for tamil : data/download/<language>/audio - this will replace <language> with tamil.\" 4. The archive_blob_path and channels_file_blob_path are folders that will be autogenerated in bucket with the given name.","title":"Bucket configuration"},{"location":"data_collection/#metadata-file-configurations","text":"Metadata file configurations in config.json mode: 'complete' This should not be changed audio_id: null If you want to give a custom audio id add here cleaned_duration: null If you know the cleaned duration of audio add here num_of_speakers: null Number of speaker present in audio language: Hindi Language of audio has_other_audio_signature: False If audio has multiple speaker in same file (True/False) type: 'audio' Type of media (audio or video) source: 'Demo_Source' Source name experiment_use: False If its for experimental use (True/False) utterances_files_list: null source_website: '' Source website url experiment_name: null Name of experiment if experiment_use is True mother_tongue: null Accent of language(Bengali, Marathi, etc...) age_group: null Age group of speaker in audio recorded_state: null State in which audio is recorded recorded_district: null District of state in which audio is recorded recorded_place: null Recording location recorded_date: null Recording date purpose: null Purpose of recording speaker_gender: null Gender of speaker speaker_name: null Name of speaker Note: 1. If any of the field info is not available keep its value to null 2. If speaker_name or speaker_gender is given then that same will be used for all the files in given source","title":"Metadata file configurations"},{"location":"data_collection/#youtube-download-configurations","text":"You can set download mode [file/channel] in youtube_pipeline_config.py mode = 'file' # [channel,file] In file mode you will store a csv file whose name must be same as source name in scraped_data_blob_path. csv must contain urls of youtube videos, speaker name and gender as three different columns. Urls is a must field. You can leave speaker name and gender blank if data is not available. Given below is the structure of csv. video_url,speaker_name,speaker_gender https://www.youtube.com/watch?v=K1vW_ZikA5o,Ram_Singh,male https://www.youtube.com/watch?v=o82HIOgozi8,John_Doe,male ... Common configurations in youtube_pipeline_config.py # Common configurations \"source_name\": \"DEMO\", This is the name of source you are downloading batch_num = 1 Number of videos to be downloaded as batches youtube_service_to_use = YoutubeService.YOUTUBE_DL This field is to choose which service to use for getting video information only_creative_commons = False Should Download only creative commons(True, False) Possible values for youtube_service_to_use: (YoutubeService.YOUTUBE_DL, YoutubeService.YOUTUBE_API) * File mode configurations in youtube_pipeline_config.py # File Mode configurations file_speaker_gender_column = 'speaker_gender' Gender column name in csv file file_speaker_name_column = \"speaker_name\" Speaker name column name in csv file file_url_name_column = \"video_url\" Video url column name in csv file license_column = \"license\" Video license column name in csv file channel mode configuration in youtube_pipeline_config.py # Channel mode configurations channel_url_dict = {} Channel url dictionary (This will download all the videos from the given channels with corresponding source names) Note: 1. In channel_url_dict, the keys must be the urls and values must be their channel names 2. To get list of channels from youtube API, channel_url_dict must be empty","title":"Youtube download configurations"},{"location":"data_collection/#youtube-api-configuration","text":"Automated Youtube fetching configuration in youtube_api_config.json # Youtube API configurations \"language\" : \"hindi\", Type of language for which search results are required. \"language_code\": \"hi\", Language code for the specified language. \"keywords\":[ The search keywords to be given in youtube API query \"audio\", \"speech\", \"talk\" ], \"words_to_ignore\":[ The words that are to be ignored in youtube API query \"song\", \"music\" ], \"max_results\": 20 Maximum number of channels or results that is required.","title":"Youtube API configuration"},{"location":"data_collection/#web-crawl-configuration","text":"web crawl configuration in web_crawl_config.json (Use this only for datacollector_bing and datacollector_urls spider) \"language\": \"gujarati\", Language to be crawled \"language_code\": \"gu\", Language code for the specified language. \"keywords\": [ Keywords to query \"talks audio\", \"audiobooks\", \"speeches\", ], \"word_to_ignore\": [ Words to ignore while crawling \"ieeexplore.ieee.org\", \"dl.acm.org\", \"www.microsoft.com\" ], \"extensions_to_ignore\": [ Formats/extensions to ignore while crawling \".jpeg\", \"xlsx\", \".xml\" ], \"extensions_to_include\": [ Formats/extensions to include while crawling \".mp3\", \".wav\", \".mp4\", ], \"pages\": 1, Number of pages to crawl \"depth\": 1, Nesting depth for each website \"continue_page\": \"NO\", Field to continue/resume crawling \"last_visited\": 200, Last visited results count \"enable_hours_restriction\": \"YES\", Restrict crawling based on hours of data collected \"max_hours\": 1 Maximum hours to crawl","title":"Web Crawl Configuration"},{"location":"data_collection/#adding-new-spider","text":"As we already mentioned our framework is extensible for any new source. To add a new source user just need to write a spider for that source. To add a spider you can follow the scrapy documentation or you can check our sample spider.","title":"Adding new spider"},{"location":"data_collection/#running-services","text":"Make sure the google credentials are present in project root folder in credentials.json file.","title":"Running services"},{"location":"data_collection/#youtube-spider-in-channel-mode","text":"In data_acqusition_framework/configs , do the following: Open config.json and change language and type to your respective use case. Open storage_config.json and change bucket and channel_blob_path to your respective gcp paths.(For more info on these fields, scroll above to Bucket configuration) Open youtube_pipeline_config.py and change mode to channel (eg: mode='channel') There are two ways to download videos of youtube channels: You can hardcode the channel url and channel name. You can use youtube-utils service(youtube-dl/youtube data api) to fetch channels and its respective videos information. To download by hardcoding the channel urls, do the following: Open data_acqusition_framework/configs/youtube_pipeline_config.py and do the following: Add the channel_urls and its names in channel_url_dict variable. eg. channel_url_dict = { \"https://www.youtube.com/channel/UC2XEzs5R1mn2wTKgtjuMxiQ\": \"channel_name_a\", \"https://www.youtube.com/channel/UC2XEzs5R1mn2wTKgtjuMxiQ\":\"channel_name_b\" } Set youtube_service_to_use variable value to either YoutubeService.YOUTUBE_DL or YoutubeService.YOUTUBE_API for collecting video info. If YoutubeService.YOUTUBE_API is chosen, then get APIKEY for youtube data api from google developer console and store it in a file called .youtube_api_key in project root folder. From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos from youtube for the given channels and download them to bucket. To download by using youtube-utils service, do the following: Open data_acqusition_framework/configs/youtube_pipeline_config.py and do the following: Assign channel_url_dict = {} (If not empty, will not work). Set youtube_service_to_use variable value to either YoutubeService.YOUTUBE_DL or YoutubeService.YOUTUBE_API for collecting video info. If YoutubeService.YOUTUBE_API is chosen, then get APIKEY for youtube data api from google developer console and store it in a file called .youtube_api_key in project root folder. Open data_acqusition_framework/configs/youtube_api_config.json and change the fields to your requirements.(For more info: check above in Youtube api configuration) From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos from youtube for the given channels and download them to bucket.","title":"Youtube spider in channel mode:"},{"location":"data_collection/#youtube-spider-in-file-mode","text":"In data_acqusition_framework/configs , do the following: Open config.json and change language and type to your respective use case. Open storage_config.json and change bucket and channel_blob_path to your respective gcp paths.(For more info on these fields, scroll above to Bucket configuration) Open youtube_pipeline_config.py and do the following: change mode to file (eg: mode='file'). change source_name to your requirement so that videos get downloaded to that folder in google storage bucket. Next Steps: Create a file in the following format: eg. source_name.csv with content (license column is optional): Here source_name in source_name.csv is the name you gave in youtube_pipeline_config.py file. It should be the same. video_url,speaker_name,speaker_gender,license https://www.youtube.com/watch?v=K1vW_ZikA5o,Ram_Singh,male,Creative Commons https://www.youtube.com/watch?v=o82HIOgozi8,John_Doe,male,Standard Youtube ... Now to upload this file to google cloud storage do the following: Open the channel_blob_path folder that you gave in storage_config.json and create a folder there named data_to_be_scraped . Upload the file that you created with previous step to this folder. From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos mentioned in the file from youtube and download them to bucket.","title":"Youtube spider in file mode:"},{"location":"data_collection/#bing-spider","text":"Configure data_acquisition_framework/configs/web_crawl_config.json for your requirements. Starting datacollector_bing spider with audio pipeline. From project root folder, run the following: scrapy crawl datacollector_bing","title":"Bing Spider"},{"location":"data_collection/#urls-spider","text":"Configure data_acquisition_framework/configs/web_crawl_config.json for your requirements. Starting datacollector_urls spider with audio pipeline. Make sure to put the urls to crawl in the data_acquisition_framework/urls.txt . From project root folder, run the following: scrapy crawl datacollector_urls","title":"Urls Spider"},{"location":"data_collection/#selenium-google-crawler","text":"It is capable of crawling search results of google for a given language and exporting them to urls.txt file. This urls.txt file can be used with datacollector_urls spider to crawl all the search results website and download the media along with their metadata. A specified Readme can be found in selenium_google_crawler folder. Readme for selenium google crawler","title":"Selenium google crawler"},{"location":"data_collection/#selenium-youtube-crawler-for-file-mode-and-api-mode","text":"It is capable of crawling youtube videos using youtube api or from a list of files with youtube video ids provided with channel name as filename. A specified Readme can be found in selenium_youtube_crawler folder. Readme for selenium youtube crawler","title":"Selenium youtube crawler for file mode and api mode"},{"location":"data_collection/#contributing","text":"Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request","title":"Contributing"},{"location":"data_collection/#license","text":"Distributed under the [XYZ] License. See LICENSE for more information.","title":"License"},{"location":"data_collection/#contact","text":"Your Name - @your_twitter - email@example.com Project Link: https://github.com/your_username/repo_name","title":"Contact"},{"location":"data_collection/#acknowledgements","text":"Scrapy YouTube-dl TinyTag","title":"Acknowledgements"},{"location":"intelligent_data_pipelines/","text":"Intelligent Data Pipeline Table of Contents Intelligent Data Pipeline Table of Contents About The Project Getting Started Architecture Intelligent Data Pipeline - Jobs Audio Processor Audio Analysis Language identification Speaker identification Gender identification Audio Data Balancing Audio Validation Audio Transcription Installation Run on Kubernetes Using Composer Requirements Infra Setup CI/CD setup Audio Processing Description Config Steps to run Audio Analysis Config Steps to run Data Balancing config steps to run: Audio Transcription (with config): config: steps to run: Contributing License Contact About The Project Intelligent Data Pipelines are built to create the audio data set that can be used for Speech Recognition deeplearning models. The aim is to allow easy, quick and fast dataset generation without doing manual work. It splits data into smallers utterences which are understood well by deeplearning models. The data is then cleansed based on 'Signal to Noise ratio'. The audio analysis is performed using pre trained models and clustering based on audio features (see Resymbleyr for more details). It leverages Kubernetes for parallel computing and below are the metrics we have acheived so far: Some stats for a language with 1000 hrs raw data Raw data 1000 hrs Time taken: 2-3 days Final Usable Data of Pretraining: 600 Final Usable Data of Fine Tuning: 400 Getting Started The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Data Pipelines. To get started install the prerequisites and clone the repo to machine on which you wish to run the framework. Here is the code Architecture Intelligent Data Pipeline - Jobs Audio Processor Audio Processor job takes raw data generted from Data Collection Pipeline or it also consumes data that is generated by any other means. It splits data into smaller utterances and then cleanses based on 'Signal to Noise Ratio (SNR)'. The threshold can be changed through configuration. It then adds audio metadata to the catalogue (PostgresDB). Audio Analysis Audio Analysis job takes cleaned and processed data from Audio Processor job and performs three type of analysys: Language identification It predicts the language of each utterance in a source using a pre trained model for a language. It gives the confidence score of the language for each utterance. Please see this for more details. Speaker identification It estimates the total number of speakers in a source. It also maps the utterances to the speaker. That metadata is required for data balancing. Please see this for more details. Gender identification It estimates the gender of each utterance in a source using a pre trained model. Please see this for more details. ### Audio Data Balancing The model training data requires data with right ratio of gender. Also the data should be balanced based on speaker duration. It also provides capability to filter and choose data based on certain metadata filter criteria. Audio Validation The data that goes into model training should be of good quality. This job validates data that is not adhereing to quiality standards required by the model. It generates csv reports that can be analysed by data scientists to further filter out the best data for model training. Audio Transcription For model fine-tuning, the paired audio data is required (audio with labeled text). This job generates text for the utterances using Google or Azure API's. The texts generated are further sanitized based on the rules defined for the language. Installation Clone the repo git clone git@github.com:Open-Speech-EkStep/audio-to-speech-pipeline.git Install python requirements pip install -r requirements.txt Run on Kubernetes Using Composer Requirements Terraform https://www.terraform.io/downloads.html gcloud https://cloud.google.com/sdk/docs/install Infra Setup Clone the repo: sh git clone git@github.com:Open-Speech-EkStep/audio-to-speech-pipeline.git Initialize terraform modules terraform init Select a workspace as per the environments(dev,test,prod). terraform workspace select <env_name> eg: terraform workspace select prod Run specific modules as per requirements. terraform apply -target=module.<module-name> eg: terraform apply -target=module.sql-database Run all modules at once. terraform apply CI/CD setup Once you pull code you have to configure some variable in your circle-ci . So that while deploying code image should easily push into google container registry. 1. GCP_PROJECT # Name of your GCP project 2. GOOGLE_AUTH # Service account key that is created using terraform 3. POSTGRES_DB # Database host ip that is created using terraform 4. POSTGRES_PASSWORD # Database password 5. POSTGRES_USER # Database user name Audio Processing Description Config config: common: db_configuration: db_name: '' db_pass: '' db_user: '' cloud_sql_connection_name: '<DB Host>' gcs_config: # master data bucket master_bucket: '<Name of the bucket>' audio_processor_config: # feat_language_identification should true if you want run language identification for a source feat_language_identification: False # language of the audio language: '' # path of the files on gcs which need to be processed # path eg: <bucket-name/data/audiotospeech/raw/download/downloaded/{language}/audio> remote_raw_audio_file_path: '' # after processing where we want to move raw data snr_done_folder_path: '' # <bucket-name/data/audiotospeech/raw/download/snr_done/{language}/audio> # path where the processed files need to be uploaded remote_processed_audio_file_path: '' # <bucket-name/data/audiotospeech/raw/download/catalogue/{language}/audio> # path where Duplicate files need to be uploaded based on checksum duplicate_audio_file_path: '' # <bucket-name/data/audiotospeech/raw/download/duplicate/{language}/audio> chunking_conversion_configuration: aggressiveness: '' # using for vad by default it's value is 2 the more the value that aggressive vad for chunking audio max_duration: '' # max duration is second if chunk is more than that vad will retry chunking with inc aggressiveness # SNR specific configurations snr_configuration: max_snr_threshold: '' # less than max_snr_threshold utterance will move to rejected folder. local_input_file_path: '' local_output_file_path: '' Steps to run We have to configure sourcepathforsnr in airflow variable where our raw data stored. Other variable is snrcatalogue in that we update our source which we want to run and count how many file should run in one trigger.and format is what raw audio file format in bucket and language and parallelism is how many pod will up in one run if parallelism is not define number of pod = count ex: ```json \"snrcatalogue\": { \" \": { \"count\": 5, \"format\": \"mp3\", \"language\": \"telugu\", \"parallelism\":2 } * We have to also set **audiofilelist** with whatever source we want to run with empty array that will store our file path ex: ```json \"audiofilelist\": { \"<source_name>\": [] } ``` * That will create a dag with the source_name now we can trigger that dag that will process given number(count) of file. and upload processed file to **remote_processed_audio_file_path** that we mentioned in config file. and move raw data from **remote_raw_audio_file_path** to **snr_done_folder_path**. and update DB also with the metadata which we created using circle-ci. ### Audio Analysis #### Config ```yaml audio_analysis_config: analysis_options: gender_analysis: 1 # It should be 1 or 0 if you want to run gender_analysis it should be 1 else 0 speaker_analysis: 0 # It should be 1 or 0 if you want to run speaker_analysis it should be 1 else 0 # path where the processed files need to be uploaded remote_processed_audio_file_path: '<bucket_name>/data/audiotospeech/raw/download/catalogued/{language}/audio' # speaker_analysis_config it's for gender_analysis module speaker_analysis_config: min_cluster_size: 4 # min_cluster_size is least number of cluster for one speaker partial_set_size: 8000 # number of utterances for create embeddings for a given source fit_noise_on_similarity: 0.77 min_samples: 2 ``` #### Steps to run * We have to configure **audio_analysis_config** in airflow variable in this json we have to mention source name and language. ```json \"audio_analysis_config\" : { \"<source name>\" : { \"language\" : \"hindi\" } } That will create a dag audio_analysis now we can trigger that dag that will process given sources. and upload processed file to remote_processed_audio_file_path that we mentioned in config file. and update DB also with the metadata which we created using circle-ci. Data Balancing config data_tagger_config: # path of to the folder in the master bucket where the data tagger will move the data to landing_directory_path: '' #'<bucket_name>/data/audiotospeech/raw/download/catalogued/{language}/audio' # path of to the folder in the master bucket from where the data tagger will pick up the data that needs to be moved source_directory_path: '' #'<bucket_name>/data/audiotospeech/raw/landing/{language}/audio' steps to run: We need to configure data_filter_config airflow variable for each source. we have multiple filters by_snr # filter based on SNR value by_duration # total duration from a given source. by_speaker # we can configure how much data per speaker we want. by_utterance_duration # we can required duration of utterance. exclude_audio_ids # we can pass a list of audio_ids that we want to skip. exclude_speaker_ids # we can pass a list of speaker_ids that we want to skip. with_randomness # It is a boolean value if it's it will pickup random data from DB. \"data_filter_config\": { \"test_source1\": { \"language\": \"hindi\", \"filter\": { \"by_snr\": { \"lte\": 75, \"gte\": 15 }, \"by_duration\": 2, \"with_randomness\": \"true\" } }, \"test_source2\": { \"language\": \"hindi\", \"filter\": { \"by_speaker\": { \"lte_per_speaker_duration\": 60, \"gte_per_speaker_duration\": 0, \"with_threshold\": 0 }, \"by_duration\": 2 } } After configure all value one dag will created data_marker_pipeline we can trigger that dag. this dag filter out all data from given criteria It will pick data from source_directory_path and after filtering move data to landing_directory_path . Audio Transcription (with config): config: ``` config: common: db_configuration: db_name: '' db_pass: '' db_user: '' cloud_sql_connection_name: '<DB host>' gcs_config: # master data bucket master_bucket: '<bucket name>' azure_transcription_client: speech_key: '<key of the api>' service_region: 'centralindia' # service region google_transcription_client: bucket: '<bucket name>' language: 'hi-IN' # It is BCP-47 language tag with this we call STT api. sample_rate: 16000 # Sample rate of audio utterance audio_channel_count: 1 #The number of channels in the input audio data audio_transcription_config: # defaults to hi-IN language: 'hi-IN' # language # audio_language it's used for sanitization rule whichever language you choose you need to add a rule class for the same. # You can use reference of hindi sanitization # sanitization rule eg: empty transcription, strip, char etc audio_language: 'kannada' # Bucket bath of wav file remote_clean_audio_file_path: '<bucketname>/data/audiotospeech/raw/landing/{language}/audio' # path where the processed files need to be uploaded remote_stt_audio_file_path: '<bucketname>/data/audiotospeech/integration/processed/{language}/audio' ``` steps to run: We have to configure sttsourcepath in airflow variable where our raw data stored. Other variable is sourceinfo in that we update our source which we want to run for STT and count how many file should run in one trigger.stt is whatever api we want to call for STT for google and azure we have all rapper for other API you can add rapper as well. language and parallelism is how many pod will up in one run if parallelism is not define number of pod = count ex: \"snrcatalogue\": { \"<source_name>\": { \"count\": 5, \"stt\":\"google\" \"language\": \"telugu\", \"parallelism\":2 } We have to also set audioidsforstt and integrationprocessedpath with whatever source we want to run with empty array that will store audio_id ex: json \"audioidsforstt\": { \"<source_name>\": [] } yaml integrationprocessedpath:\"\" # path of folder where we want move transcribed data. That will create a dag with the source_name now we can trigger that dag that will process given number(count) of file. and upload processed file to remote_stt_audio_file_path that we mentioned in config file. and move raw data from remote_clean_audio_file_path to integrationprocessedpath . and update DB also with the metadata which we created using circle-ci. Contributing Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request We follow conventional commits License Distributed under the [MIT] License. See LICENSE for more information. Contact Your Name - @your_twitter - email@example.com Project Link: https://github.com/Open-Speech-EkStep/audio-to-speech-pipeline","title":"Intelligent Data Pipeline"},{"location":"intelligent_data_pipelines/#intelligent-data-pipeline","text":"","title":"Intelligent Data Pipeline"},{"location":"intelligent_data_pipelines/#table-of-contents","text":"Intelligent Data Pipeline Table of Contents About The Project Getting Started Architecture Intelligent Data Pipeline - Jobs Audio Processor Audio Analysis Language identification Speaker identification Gender identification Audio Data Balancing Audio Validation Audio Transcription Installation Run on Kubernetes Using Composer Requirements Infra Setup CI/CD setup Audio Processing Description Config Steps to run Audio Analysis Config Steps to run Data Balancing config steps to run: Audio Transcription (with config): config: steps to run: Contributing License Contact","title":"Table of Contents"},{"location":"intelligent_data_pipelines/#about-the-project","text":"Intelligent Data Pipelines are built to create the audio data set that can be used for Speech Recognition deeplearning models. The aim is to allow easy, quick and fast dataset generation without doing manual work. It splits data into smallers utterences which are understood well by deeplearning models. The data is then cleansed based on 'Signal to Noise ratio'. The audio analysis is performed using pre trained models and clustering based on audio features (see Resymbleyr for more details). It leverages Kubernetes for parallel computing and below are the metrics we have acheived so far: Some stats for a language with 1000 hrs raw data Raw data 1000 hrs Time taken: 2-3 days Final Usable Data of Pretraining: 600 Final Usable Data of Fine Tuning: 400","title":"About The Project"},{"location":"intelligent_data_pipelines/#getting-started","text":"The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Data Pipelines. To get started install the prerequisites and clone the repo to machine on which you wish to run the framework. Here is the code","title":"Getting Started"},{"location":"intelligent_data_pipelines/#architecture","text":"","title":"Architecture"},{"location":"intelligent_data_pipelines/#intelligent-data-pipeline-jobs","text":"","title":"Intelligent Data Pipeline - Jobs"},{"location":"intelligent_data_pipelines/#audio-processor","text":"Audio Processor job takes raw data generted from Data Collection Pipeline or it also consumes data that is generated by any other means. It splits data into smaller utterances and then cleanses based on 'Signal to Noise Ratio (SNR)'. The threshold can be changed through configuration. It then adds audio metadata to the catalogue (PostgresDB).","title":"Audio Processor"},{"location":"intelligent_data_pipelines/#audio-analysis","text":"Audio Analysis job takes cleaned and processed data from Audio Processor job and performs three type of analysys:","title":"Audio Analysis"},{"location":"intelligent_data_pipelines/#language-identification","text":"It predicts the language of each utterance in a source using a pre trained model for a language. It gives the confidence score of the language for each utterance. Please see this for more details.","title":"Language identification"},{"location":"intelligent_data_pipelines/#speaker-identification","text":"It estimates the total number of speakers in a source. It also maps the utterances to the speaker. That metadata is required for data balancing. Please see this for more details.","title":"Speaker identification"},{"location":"intelligent_data_pipelines/#gender-identification","text":"It estimates the gender of each utterance in a source using a pre trained model. Please see this for more details. ### Audio Data Balancing The model training data requires data with right ratio of gender. Also the data should be balanced based on speaker duration. It also provides capability to filter and choose data based on certain metadata filter criteria.","title":"Gender identification"},{"location":"intelligent_data_pipelines/#audio-validation","text":"The data that goes into model training should be of good quality. This job validates data that is not adhereing to quiality standards required by the model. It generates csv reports that can be analysed by data scientists to further filter out the best data for model training.","title":"Audio Validation"},{"location":"intelligent_data_pipelines/#audio-transcription","text":"For model fine-tuning, the paired audio data is required (audio with labeled text). This job generates text for the utterances using Google or Azure API's. The texts generated are further sanitized based on the rules defined for the language.","title":"Audio Transcription"},{"location":"intelligent_data_pipelines/#installation","text":"Clone the repo git clone git@github.com:Open-Speech-EkStep/audio-to-speech-pipeline.git Install python requirements pip install -r requirements.txt","title":"Installation"},{"location":"intelligent_data_pipelines/#run-on-kubernetes","text":"","title":"Run on Kubernetes"},{"location":"intelligent_data_pipelines/#using-composer","text":"","title":"Using Composer"},{"location":"intelligent_data_pipelines/#requirements","text":"Terraform https://www.terraform.io/downloads.html gcloud https://cloud.google.com/sdk/docs/install","title":"Requirements"},{"location":"intelligent_data_pipelines/#infra-setup","text":"Clone the repo: sh git clone git@github.com:Open-Speech-EkStep/audio-to-speech-pipeline.git Initialize terraform modules terraform init Select a workspace as per the environments(dev,test,prod). terraform workspace select <env_name> eg: terraform workspace select prod Run specific modules as per requirements. terraform apply -target=module.<module-name> eg: terraform apply -target=module.sql-database Run all modules at once. terraform apply","title":"Infra Setup"},{"location":"intelligent_data_pipelines/#cicd-setup","text":"Once you pull code you have to configure some variable in your circle-ci . So that while deploying code image should easily push into google container registry. 1. GCP_PROJECT # Name of your GCP project 2. GOOGLE_AUTH # Service account key that is created using terraform 3. POSTGRES_DB # Database host ip that is created using terraform 4. POSTGRES_PASSWORD # Database password 5. POSTGRES_USER # Database user name","title":"CI/CD setup"},{"location":"intelligent_data_pipelines/#audio-processing","text":"","title":"Audio Processing"},{"location":"intelligent_data_pipelines/#description","text":"","title":"Description"},{"location":"intelligent_data_pipelines/#config","text":"config: common: db_configuration: db_name: '' db_pass: '' db_user: '' cloud_sql_connection_name: '<DB Host>' gcs_config: # master data bucket master_bucket: '<Name of the bucket>' audio_processor_config: # feat_language_identification should true if you want run language identification for a source feat_language_identification: False # language of the audio language: '' # path of the files on gcs which need to be processed # path eg: <bucket-name/data/audiotospeech/raw/download/downloaded/{language}/audio> remote_raw_audio_file_path: '' # after processing where we want to move raw data snr_done_folder_path: '' # <bucket-name/data/audiotospeech/raw/download/snr_done/{language}/audio> # path where the processed files need to be uploaded remote_processed_audio_file_path: '' # <bucket-name/data/audiotospeech/raw/download/catalogue/{language}/audio> # path where Duplicate files need to be uploaded based on checksum duplicate_audio_file_path: '' # <bucket-name/data/audiotospeech/raw/download/duplicate/{language}/audio> chunking_conversion_configuration: aggressiveness: '' # using for vad by default it's value is 2 the more the value that aggressive vad for chunking audio max_duration: '' # max duration is second if chunk is more than that vad will retry chunking with inc aggressiveness # SNR specific configurations snr_configuration: max_snr_threshold: '' # less than max_snr_threshold utterance will move to rejected folder. local_input_file_path: '' local_output_file_path: ''","title":"Config"},{"location":"intelligent_data_pipelines/#steps-to-run","text":"We have to configure sourcepathforsnr in airflow variable where our raw data stored. Other variable is snrcatalogue in that we update our source which we want to run and count how many file should run in one trigger.and format is what raw audio file format in bucket and language and parallelism is how many pod will up in one run if parallelism is not define number of pod = count ex: ```json \"snrcatalogue\": { \" \": { \"count\": 5, \"format\": \"mp3\", \"language\": \"telugu\", \"parallelism\":2 } * We have to also set **audiofilelist** with whatever source we want to run with empty array that will store our file path ex: ```json \"audiofilelist\": { \"<source_name>\": [] } ``` * That will create a dag with the source_name now we can trigger that dag that will process given number(count) of file. and upload processed file to **remote_processed_audio_file_path** that we mentioned in config file. and move raw data from **remote_raw_audio_file_path** to **snr_done_folder_path**. and update DB also with the metadata which we created using circle-ci. ### Audio Analysis #### Config ```yaml audio_analysis_config: analysis_options: gender_analysis: 1 # It should be 1 or 0 if you want to run gender_analysis it should be 1 else 0 speaker_analysis: 0 # It should be 1 or 0 if you want to run speaker_analysis it should be 1 else 0 # path where the processed files need to be uploaded remote_processed_audio_file_path: '<bucket_name>/data/audiotospeech/raw/download/catalogued/{language}/audio' # speaker_analysis_config it's for gender_analysis module speaker_analysis_config: min_cluster_size: 4 # min_cluster_size is least number of cluster for one speaker partial_set_size: 8000 # number of utterances for create embeddings for a given source fit_noise_on_similarity: 0.77 min_samples: 2 ``` #### Steps to run * We have to configure **audio_analysis_config** in airflow variable in this json we have to mention source name and language. ```json \"audio_analysis_config\" : { \"<source name>\" : { \"language\" : \"hindi\" } } That will create a dag audio_analysis now we can trigger that dag that will process given sources. and upload processed file to remote_processed_audio_file_path that we mentioned in config file. and update DB also with the metadata which we created using circle-ci.","title":"Steps to run"},{"location":"intelligent_data_pipelines/#data-balancing","text":"","title":"Data Balancing"},{"location":"intelligent_data_pipelines/#config_1","text":"data_tagger_config: # path of to the folder in the master bucket where the data tagger will move the data to landing_directory_path: '' #'<bucket_name>/data/audiotospeech/raw/download/catalogued/{language}/audio' # path of to the folder in the master bucket from where the data tagger will pick up the data that needs to be moved source_directory_path: '' #'<bucket_name>/data/audiotospeech/raw/landing/{language}/audio'","title":"config"},{"location":"intelligent_data_pipelines/#steps-to-run_1","text":"We need to configure data_filter_config airflow variable for each source. we have multiple filters by_snr # filter based on SNR value by_duration # total duration from a given source. by_speaker # we can configure how much data per speaker we want. by_utterance_duration # we can required duration of utterance. exclude_audio_ids # we can pass a list of audio_ids that we want to skip. exclude_speaker_ids # we can pass a list of speaker_ids that we want to skip. with_randomness # It is a boolean value if it's it will pickup random data from DB. \"data_filter_config\": { \"test_source1\": { \"language\": \"hindi\", \"filter\": { \"by_snr\": { \"lte\": 75, \"gte\": 15 }, \"by_duration\": 2, \"with_randomness\": \"true\" } }, \"test_source2\": { \"language\": \"hindi\", \"filter\": { \"by_speaker\": { \"lte_per_speaker_duration\": 60, \"gte_per_speaker_duration\": 0, \"with_threshold\": 0 }, \"by_duration\": 2 } } After configure all value one dag will created data_marker_pipeline we can trigger that dag. this dag filter out all data from given criteria It will pick data from source_directory_path and after filtering move data to landing_directory_path .","title":"steps to run:"},{"location":"intelligent_data_pipelines/#audio-transcription-with-config","text":"","title":"Audio Transcription (with config):"},{"location":"intelligent_data_pipelines/#config_2","text":"``` config: common: db_configuration: db_name: '' db_pass: '' db_user: '' cloud_sql_connection_name: '<DB host>' gcs_config: # master data bucket master_bucket: '<bucket name>' azure_transcription_client: speech_key: '<key of the api>' service_region: 'centralindia' # service region google_transcription_client: bucket: '<bucket name>' language: 'hi-IN' # It is BCP-47 language tag with this we call STT api. sample_rate: 16000 # Sample rate of audio utterance audio_channel_count: 1 #The number of channels in the input audio data audio_transcription_config: # defaults to hi-IN language: 'hi-IN' # language # audio_language it's used for sanitization rule whichever language you choose you need to add a rule class for the same. # You can use reference of hindi sanitization # sanitization rule eg: empty transcription, strip, char etc audio_language: 'kannada' # Bucket bath of wav file remote_clean_audio_file_path: '<bucketname>/data/audiotospeech/raw/landing/{language}/audio' # path where the processed files need to be uploaded remote_stt_audio_file_path: '<bucketname>/data/audiotospeech/integration/processed/{language}/audio' ```","title":"config:"},{"location":"intelligent_data_pipelines/#steps-to-run_2","text":"We have to configure sttsourcepath in airflow variable where our raw data stored. Other variable is sourceinfo in that we update our source which we want to run for STT and count how many file should run in one trigger.stt is whatever api we want to call for STT for google and azure we have all rapper for other API you can add rapper as well. language and parallelism is how many pod will up in one run if parallelism is not define number of pod = count ex: \"snrcatalogue\": { \"<source_name>\": { \"count\": 5, \"stt\":\"google\" \"language\": \"telugu\", \"parallelism\":2 } We have to also set audioidsforstt and integrationprocessedpath with whatever source we want to run with empty array that will store audio_id ex: json \"audioidsforstt\": { \"<source_name>\": [] } yaml integrationprocessedpath:\"\" # path of folder where we want move transcribed data. That will create a dag with the source_name now we can trigger that dag that will process given number(count) of file. and upload processed file to remote_stt_audio_file_path that we mentioned in config file. and move raw data from remote_clean_audio_file_path to integrationprocessedpath . and update DB also with the metadata which we created using circle-ci.","title":"steps to run:"},{"location":"intelligent_data_pipelines/#contributing","text":"Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request We follow conventional commits","title":"Contributing"},{"location":"intelligent_data_pipelines/#license","text":"Distributed under the [MIT] License. See LICENSE for more information.","title":"License"},{"location":"intelligent_data_pipelines/#contact","text":"Your Name - @your_twitter - email@example.com Project Link: https://github.com/Open-Speech-EkStep/audio-to-speech-pipeline","title":"Contact"},{"location":"model_training/","text":"Pretrained Models We are releasing pretrained models in various Indic Languages. Please head over to this repo . Table of contents * Installation and Setup * Directory Structure * Data Description * Usage * For Pretraining * For Finetuning * For Inference * For Single File Inference * License Installation and Setup git clone https://github.com/Open-Speech-EkStep/vakyansh-wav2vec2-experimentation.git conda create --name <env_name> python=3.7 conda activate <env_name> cd vakyansh-wav2vec2-experimentation ### Packages pip install packaging soundfile swifter pip install -r requirements.txt pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html ### For fairseq setup(fairseq should be installed outside vakyansh-wav2vec2-experimentation repo) cd .. git clone -b ekstep-wav2vec2 https://github.com/Open-Speech-EkStep/fairseq.git cd fairseq pip install -e . ### install other libraries ### For Kenlm, openblas cd .. sudo apt-get install liblzma-dev libbz2-dev libzstd-dev libsndfile1-dev libopenblas-dev libfftw3-dev libgflags-dev libgoogle-glog-dev sudo apt install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev git clone https://github.com/kpu/kenlm.git cd kenlm mkdir -p build && cd build cmake .. make -j 16 cd .. export KENLM_ROOT_DIR=$PWD export USE_CUDA=0 ## for cpu cd .. ### wav2letter git clone -b v0.2 https://github.com/facebookresearch/wav2letter.git cd wav2letter git checkout b1d1f89f586120a978a4666cffd45c55f0a2e564 cd bindings/python pip install -e . Directory Structure root-directory . |-- ./checkpoints | |-- ./checkpoints/custom_model | | `-- ./checkpoints/custom_model/ | |-- ./checkpoints/finetuning | | `-- ./checkpoints/finetuning/ | `-- ./checkpoints/pretraining | `-- ./checkpoints/pretraining/ |-- ./data | |-- ./data/finetuning | | `-- ./data/finetuning/ | |-- ./data/inference | | `-- ./data/inference/ | |-- ./data/pretraining | | `-- ./data/pretraining/ | `-- ./data/processed | `-- ./data/processed/ |-- ./lm | `-- ./lm/ |-- ./logs | |-- ./logs/finetuning | | `-- ./logs/finetuning/ | `-- ./logs/pretraining | `-- ./logs/pretraining/ |-- ./notebooks | `-- ./notebooks/ |-- ./results | `-- ./results/ |-- ./scripts | |-- ./scripts/data | | `-- ./scripts/data/ | |-- ./scripts/parse_yaml.sh | |-- ./scripts/finetuning | | |-- ./scripts/finetuning/start_finetuning.sh | | |-- ./scripts/finetuning/prepare_data.sh | | `-- ./scripts/finetuning/README.md | |-- ./scripts/lm | | |-- ./scripts/lm/run_lm_pipeline.sh | | `-- ./scripts/lm/README.md | |-- ./scripts/pretraining | | |-- ./scripts/pretraining/start_pretraining_base.sh | | |-- ./scripts/pretraining/start_pretraining_large.sh | | |-- ./scripts/pretraining/prepare_data.sh | | `-- ./scripts/pretraining/README.md | `-- ./scripts/inference | |-- ./scripts/inference/infer.sh | |-- ./scripts/inference/prepare_data.sh | |-- ./scripts/inference/generate_custom_model.sh | |-- ./scripts/inference/single_file_inference.sh | `-- ./scripts/inference/README.md |-- ./config | |-- ./config/finetuning.yaml | |-- ./config/pretraining_base.yaml | |-- ./config/pretraining_large.yaml | `-- ./config/README.md |-- ./requirements.txt |-- ./utils | |-- ./utils/analysis | | `-- ./utils/analysis/generate_wav_report_from_tsv.py | |-- ./utils/prep_scripts | | |-- ./utils/prep_scripts/dict_and_lexicon_maker.py | | |-- ./utils/prep_scripts/labels.py | | `-- ./utils/prep_scripts/manifest.py | |-- ./utils/wer | | |-- ./utils/wer/wer.py | | `-- ./utils/wer/wer_wav2vec.py | |-- ./utils/inference | | |-- ./utils/inference/generate_custom_model.py | | `-- ./utils/inference/single_file_inference.py | `-- ./utils/lm | |-- ./utils/lm/concatenate_text.py | |-- ./utils/lm/make_lexicon_lst.py | |-- ./utils/lm/generate_lm.py | |-- ./utils/lm/clean_text.py | `-- ./utils/lm/remove_duplicate_lines.py `-- ./README.md Data Description For Audio Files. Sample Rate [Hz] = 16000 Channels = 'mono' Bit Rate [kbit/s] = 256 Precision [bits] = 16 Audio length should be less than 30 seconds otherwise it will be ignored during data preparation After scripts/finetuning/prepare_data.sh is run, analysis will be generated which can be used to tune min/max_sample_size in the config files For Text Files Corresponding text file of each audio file must be on the same directory as its audio Text file should not contain any punctuation characters Check dict.ltr.txt file generated after prepare_data so that it does not contain any foreign language character For Language Model Character set of text used for language model should be same as character set used for training Sample code for cleaning text file for english language is given here clean_text.py Sample code for removing duplicate line from text file is given here remove_duplicate_lines.py Usage For Pretraining Edit the path to data in the scripts/pretraining/prepare_data.sh file. To prepare the data: $ cd scripts/pretraining $ bash prepare_data.sh Edit the config/pretraining_base.yaml or config/pretraining_large.yaml for different parameter configurations.Check the required paths and values in start_pretraining_base.sh or start_pretraining_large.sh. Refer to config README To start run: $ bash start_pretraining_base.sh Refer this for pretraining parameters. For Finetuning Edit the path to data in the scripts/finetuning/prepare_data.sh file. To prepare the data: $ cd scripts/finetuning $ bash prepare_data.sh Edit the config/finetuning.yaml for different parameter configurations.Check the required paths and values in start_finetuning.sh. Refer to config README To start run: $ bash start_finetuning.sh Refer this for finetuning parameters. For Inference Edit the path to data in the scripts/inference/prepare_data.sh file. To prepare the test data run: $ cd scripts/inference/ $ bash prepare_data.sh Edit the infer.sh file for required paths. To start inference run: $ bash infer.sh Refer this for inference parameters. For Single File Inference To generate custom model, run: $ cd scripts/inference $ bash generate_custom_model.sh To infer for single file, change path in single_file_inference.sh. Then run: $ bash single_file_inference.sh For generating LM Edit the run_lm_pipeline.sh variables as required, then run: $ cd scripts/lm $ bash run_lm_pipeline.sh Refer this for LM pipeline. License fairseq(-py) is MIT-licensed. The license applies to the pre-trained models as well.","title":"Model Training Pipeline"},{"location":"model_training/#pretrained-models","text":"We are releasing pretrained models in various Indic Languages. Please head over to this repo .","title":"Pretrained Models"},{"location":"model_training/#table-of-contents","text":"* Installation and Setup * Directory Structure * Data Description * Usage * For Pretraining * For Finetuning * For Inference * For Single File Inference * License","title":"Table of contents"},{"location":"model_training/#installation-and-setup","text":"git clone https://github.com/Open-Speech-EkStep/vakyansh-wav2vec2-experimentation.git conda create --name <env_name> python=3.7 conda activate <env_name> cd vakyansh-wav2vec2-experimentation ### Packages pip install packaging soundfile swifter pip install -r requirements.txt pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html ### For fairseq setup(fairseq should be installed outside vakyansh-wav2vec2-experimentation repo) cd .. git clone -b ekstep-wav2vec2 https://github.com/Open-Speech-EkStep/fairseq.git cd fairseq pip install -e . ### install other libraries ### For Kenlm, openblas cd .. sudo apt-get install liblzma-dev libbz2-dev libzstd-dev libsndfile1-dev libopenblas-dev libfftw3-dev libgflags-dev libgoogle-glog-dev sudo apt install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev git clone https://github.com/kpu/kenlm.git cd kenlm mkdir -p build && cd build cmake .. make -j 16 cd .. export KENLM_ROOT_DIR=$PWD export USE_CUDA=0 ## for cpu cd .. ### wav2letter git clone -b v0.2 https://github.com/facebookresearch/wav2letter.git cd wav2letter git checkout b1d1f89f586120a978a4666cffd45c55f0a2e564 cd bindings/python pip install -e .","title":"Installation and Setup"},{"location":"model_training/#directory-structure","text":"root-directory . |-- ./checkpoints | |-- ./checkpoints/custom_model | | `-- ./checkpoints/custom_model/ | |-- ./checkpoints/finetuning | | `-- ./checkpoints/finetuning/ | `-- ./checkpoints/pretraining | `-- ./checkpoints/pretraining/ |-- ./data | |-- ./data/finetuning | | `-- ./data/finetuning/ | |-- ./data/inference | | `-- ./data/inference/ | |-- ./data/pretraining | | `-- ./data/pretraining/ | `-- ./data/processed | `-- ./data/processed/ |-- ./lm | `-- ./lm/ |-- ./logs | |-- ./logs/finetuning | | `-- ./logs/finetuning/ | `-- ./logs/pretraining | `-- ./logs/pretraining/ |-- ./notebooks | `-- ./notebooks/ |-- ./results | `-- ./results/ |-- ./scripts | |-- ./scripts/data | | `-- ./scripts/data/ | |-- ./scripts/parse_yaml.sh | |-- ./scripts/finetuning | | |-- ./scripts/finetuning/start_finetuning.sh | | |-- ./scripts/finetuning/prepare_data.sh | | `-- ./scripts/finetuning/README.md | |-- ./scripts/lm | | |-- ./scripts/lm/run_lm_pipeline.sh | | `-- ./scripts/lm/README.md | |-- ./scripts/pretraining | | |-- ./scripts/pretraining/start_pretraining_base.sh | | |-- ./scripts/pretraining/start_pretraining_large.sh | | |-- ./scripts/pretraining/prepare_data.sh | | `-- ./scripts/pretraining/README.md | `-- ./scripts/inference | |-- ./scripts/inference/infer.sh | |-- ./scripts/inference/prepare_data.sh | |-- ./scripts/inference/generate_custom_model.sh | |-- ./scripts/inference/single_file_inference.sh | `-- ./scripts/inference/README.md |-- ./config | |-- ./config/finetuning.yaml | |-- ./config/pretraining_base.yaml | |-- ./config/pretraining_large.yaml | `-- ./config/README.md |-- ./requirements.txt |-- ./utils | |-- ./utils/analysis | | `-- ./utils/analysis/generate_wav_report_from_tsv.py | |-- ./utils/prep_scripts | | |-- ./utils/prep_scripts/dict_and_lexicon_maker.py | | |-- ./utils/prep_scripts/labels.py | | `-- ./utils/prep_scripts/manifest.py | |-- ./utils/wer | | |-- ./utils/wer/wer.py | | `-- ./utils/wer/wer_wav2vec.py | |-- ./utils/inference | | |-- ./utils/inference/generate_custom_model.py | | `-- ./utils/inference/single_file_inference.py | `-- ./utils/lm | |-- ./utils/lm/concatenate_text.py | |-- ./utils/lm/make_lexicon_lst.py | |-- ./utils/lm/generate_lm.py | |-- ./utils/lm/clean_text.py | `-- ./utils/lm/remove_duplicate_lines.py `-- ./README.md","title":"Directory Structure"},{"location":"model_training/#data-description","text":"For Audio Files. Sample Rate [Hz] = 16000 Channels = 'mono' Bit Rate [kbit/s] = 256 Precision [bits] = 16 Audio length should be less than 30 seconds otherwise it will be ignored during data preparation After scripts/finetuning/prepare_data.sh is run, analysis will be generated which can be used to tune min/max_sample_size in the config files For Text Files Corresponding text file of each audio file must be on the same directory as its audio Text file should not contain any punctuation characters Check dict.ltr.txt file generated after prepare_data so that it does not contain any foreign language character For Language Model Character set of text used for language model should be same as character set used for training Sample code for cleaning text file for english language is given here clean_text.py Sample code for removing duplicate line from text file is given here remove_duplicate_lines.py","title":"Data Description"},{"location":"model_training/#usage","text":"","title":"Usage"},{"location":"model_training/#for-pretraining","text":"Edit the path to data in the scripts/pretraining/prepare_data.sh file. To prepare the data: $ cd scripts/pretraining $ bash prepare_data.sh Edit the config/pretraining_base.yaml or config/pretraining_large.yaml for different parameter configurations.Check the required paths and values in start_pretraining_base.sh or start_pretraining_large.sh. Refer to config README To start run: $ bash start_pretraining_base.sh Refer this for pretraining parameters.","title":"For Pretraining"},{"location":"model_training/#for-finetuning","text":"Edit the path to data in the scripts/finetuning/prepare_data.sh file. To prepare the data: $ cd scripts/finetuning $ bash prepare_data.sh Edit the config/finetuning.yaml for different parameter configurations.Check the required paths and values in start_finetuning.sh. Refer to config README To start run: $ bash start_finetuning.sh Refer this for finetuning parameters.","title":"For Finetuning"},{"location":"model_training/#for-inference","text":"Edit the path to data in the scripts/inference/prepare_data.sh file. To prepare the test data run: $ cd scripts/inference/ $ bash prepare_data.sh Edit the infer.sh file for required paths. To start inference run: $ bash infer.sh Refer this for inference parameters.","title":"For Inference"},{"location":"model_training/#for-single-file-inference","text":"To generate custom model, run: $ cd scripts/inference $ bash generate_custom_model.sh To infer for single file, change path in single_file_inference.sh. Then run: $ bash single_file_inference.sh","title":"For Single File Inference"},{"location":"model_training/#for-generating-lm","text":"Edit the run_lm_pipeline.sh variables as required, then run: $ cd scripts/lm $ bash run_lm_pipeline.sh Refer this for LM pipeline.","title":"For generating LM"},{"location":"model_training/#license","text":"fairseq(-py) is MIT-licensed. The license applies to the pre-trained models as well.","title":"License"},{"location":"speaker_clustering/","text":"Speaker Clustering Table of Contents Speaker Clustering Table of Contents About The Project Working Embeddings Clustering algorithm Merging and Splitting Fitting Noise points Hyperparameters About The Project Speaker Clustering, or identification of spekaers in the wild is mainly useful for audio sources with no mapping between audios and a speaker label/name. It is the task of identifying the unique speakers in a set of audio recordings (each belonging to exactly one speaker) without knowing who and how many speakers are present in the entire data. Our intelligent data pipelines split each audio, based on Voice Activity Detection, as one of the starting steps. These shorter audio utterances are then used to train deep learning models. We assume that the utterances are short enough to have one speaker per utterance, since the splitting logic is using unvoiced segments as points to split an audio. Working This documentation will help you understand the various steps involved and take you through the hyperparameters, so you can tune them to achieve the best possible results. Embeddings We use the Voice Encoder model proposed here , and implemented here for converting our audio utterances into fixed length embeddings. Voice Encoder is speaker-discriminative model trained on a text-independent speaker verification task. Thus it allows us to derive a high-level representation of the voice present in an audio. An embedding is a 256 dimensional vector capable of summarizing the characteristics of the voice spoken. Clustering algorithm Embeddings for a source are passed as a matrix for clustering to be performed using HDBSCAN. If the number of embeddings are greater than the parameter partial_set_size , they are divided into multiple partial sets of this size. This step is done to reduce the computational cost of calculating cosine distances and other internal matrices in HDBSCAN for a matrix. We use Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) as our core clustering algorithm on each of these partial steps. This step also classifies some points as noise points - meaning they couldn't be used up in any clusters formed for this partial set. We keep a record of all these noise points for each partial set. Merging and Splitting We found in our experiments that some of the speakers had their clusters distributed as separate ones - even in one partial set. This step helps in allowing such clusters to merge. Merging is based on cosine similarity (94-96% similar clusters are merged repetitively). Also, clusters for the same speaker but from different partial sets also get merged in this step. For some sources in our experiments, big clusters usually contained very high diversity of speakers. Splitting is done on such bigger clusters to make sure we have high cluster purity. A cluster is called big if it has more than 3 times the average number of points across all clusters. We also tried splitting a large cluster containing audios from one spekaer only and it was retianed as is. So this step should not affect large clusters for one speaker. Splitting is achieved by using Leaf HDBSCAN clustering on the big clusters. This allows for more more fine grained clustering. Repetitive merging is applied again after splitting to allow clusters with high cosine simialrities to be merged. Fitting Noise points All the noise points - points which could not be put into a cluster, are allowed to merge with a clusters if they have a cosine similairy of >= 80% with a noise point. This parameter is also configurable. Hyperparameters min_cluster_size: the smallest size grouping that you wish to consider a cluster min_samples: number of points required in the neighborhood of a point to be considered a core point smallest value = 1, max value = min_cluster_size smaller values of min_samples have an effect of lowering the number of noise points partial_set_size: number of utterances to treat as one set for clustering. fit_noise_on_similarity: cosine similarity between a noise point and a cluster, at which the point can be fit to a cluster. cluster_selection_method: can either be \u2018eom\u2019 or \u2018leaf\u2019 'eom' or Excess of Mass is the default way of HDBSCAN working. 'leaf' will select leaf nodes from the tree, producing many small homogeneous clusters. Allowing for fine-grained clusters. This is used while splitting clsuters into smaller ones.","title":"Speaker Clustering"},{"location":"speaker_clustering/#speaker-clustering","text":"","title":"Speaker Clustering"},{"location":"speaker_clustering/#table-of-contents","text":"Speaker Clustering Table of Contents About The Project Working Embeddings Clustering algorithm Merging and Splitting Fitting Noise points Hyperparameters","title":"Table of Contents"},{"location":"speaker_clustering/#about-the-project","text":"Speaker Clustering, or identification of spekaers in the wild is mainly useful for audio sources with no mapping between audios and a speaker label/name. It is the task of identifying the unique speakers in a set of audio recordings (each belonging to exactly one speaker) without knowing who and how many speakers are present in the entire data. Our intelligent data pipelines split each audio, based on Voice Activity Detection, as one of the starting steps. These shorter audio utterances are then used to train deep learning models. We assume that the utterances are short enough to have one speaker per utterance, since the splitting logic is using unvoiced segments as points to split an audio.","title":"About The Project"},{"location":"speaker_clustering/#working","text":"This documentation will help you understand the various steps involved and take you through the hyperparameters, so you can tune them to achieve the best possible results.","title":"Working"},{"location":"speaker_clustering/#embeddings","text":"We use the Voice Encoder model proposed here , and implemented here for converting our audio utterances into fixed length embeddings. Voice Encoder is speaker-discriminative model trained on a text-independent speaker verification task. Thus it allows us to derive a high-level representation of the voice present in an audio. An embedding is a 256 dimensional vector capable of summarizing the characteristics of the voice spoken.","title":"Embeddings"},{"location":"speaker_clustering/#clustering-algorithm","text":"Embeddings for a source are passed as a matrix for clustering to be performed using HDBSCAN. If the number of embeddings are greater than the parameter partial_set_size , they are divided into multiple partial sets of this size. This step is done to reduce the computational cost of calculating cosine distances and other internal matrices in HDBSCAN for a matrix. We use Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) as our core clustering algorithm on each of these partial steps. This step also classifies some points as noise points - meaning they couldn't be used up in any clusters formed for this partial set. We keep a record of all these noise points for each partial set.","title":"Clustering algorithm"},{"location":"speaker_clustering/#merging-and-splitting","text":"We found in our experiments that some of the speakers had their clusters distributed as separate ones - even in one partial set. This step helps in allowing such clusters to merge. Merging is based on cosine similarity (94-96% similar clusters are merged repetitively). Also, clusters for the same speaker but from different partial sets also get merged in this step. For some sources in our experiments, big clusters usually contained very high diversity of speakers. Splitting is done on such bigger clusters to make sure we have high cluster purity. A cluster is called big if it has more than 3 times the average number of points across all clusters. We also tried splitting a large cluster containing audios from one spekaer only and it was retianed as is. So this step should not affect large clusters for one speaker. Splitting is achieved by using Leaf HDBSCAN clustering on the big clusters. This allows for more more fine grained clustering. Repetitive merging is applied again after splitting to allow clusters with high cosine simialrities to be merged.","title":"Merging and Splitting"},{"location":"speaker_clustering/#fitting-noise-points","text":"All the noise points - points which could not be put into a cluster, are allowed to merge with a clusters if they have a cosine similairy of >= 80% with a noise point. This parameter is also configurable.","title":"Fitting Noise points"},{"location":"speaker_clustering/#hyperparameters","text":"min_cluster_size: the smallest size grouping that you wish to consider a cluster min_samples: number of points required in the neighborhood of a point to be considered a core point smallest value = 1, max value = min_cluster_size smaller values of min_samples have an effect of lowering the number of noise points partial_set_size: number of utterances to treat as one set for clustering. fit_noise_on_similarity: cosine similarity between a noise point and a cluster, at which the point can be fit to a cluster. cluster_selection_method: can either be \u2018eom\u2019 or \u2018leaf\u2019 'eom' or Excess of Mass is the default way of HDBSCAN working. 'leaf' will select leaf nodes from the tree, producing many small homogeneous clusters. Allowing for fine-grained clusters. This is used while splitting clsuters into smaller ones.","title":"Hyperparameters"}]}