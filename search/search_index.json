{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Vakyansh Recipies to build Speech Recognition models Introduction Vakyansh aims to host the key essentials of Automatic Speech Recognition (ASR) technology, focusing on Indian languages. It is a resource that allows people to build applications that leverage speech recognition. The site will host open data for training ASR models, open source utilities and pipelines to train ASR models and open ASR models themselves. Vakyansh also hosts tools to contribute your voices to create a diverse open data repository of Indian voices to this end. This data will be available in an open manner for all to use. It is a resource that allows people to build applications that leverage speech recognition. Open Source Open sourcing the speech recognition technology empowers us to bring our languages to the core of our fundamentals. Vakyansh aims to open source the speech recognition models in various languages, the datasets collected through various channels and the linguistic utilities developed to process and clean the data and make it usable by speech recognition tools. The open source strategy will enable the various language communities, individuals and technologists, who are passionate about their language, to develop speech recognition applications, and integrate them in various domains for the good of the community. Getting started Vakyansh's developer documentation is meant for its adopters, developers and contributors. It would enable people to innovate and improve and build Speech Recognition Models in any language. Modeling Process Vakyansh Components 1. Data Collection Pipeline Pipelines to collect data in automated way for the language you want 2. Intelligent Data Pipeline Pipelines to transform raw data and prepare data for model training. They clean, process and balance data for model training 3. Model Training Pipeline Pipeline to build state of the art Speech Recognition Model using the data provided by Intelligent Data Pipeline Vakyansh Technical Overview The Developer documentation provides you with a complete set of guidelines which you need to: Install Vakyansh Configure Vakyansh Customize Vakyansh Extend Vakyansh Contribute to Vakyansh","title":"Home"},{"location":"#welcome-to-vakyansh","text":"Recipies to build Speech Recognition models","title":"Welcome to Vakyansh"},{"location":"#introduction","text":"Vakyansh aims to host the key essentials of Automatic Speech Recognition (ASR) technology, focusing on Indian languages. It is a resource that allows people to build applications that leverage speech recognition. The site will host open data for training ASR models, open source utilities and pipelines to train ASR models and open ASR models themselves. Vakyansh also hosts tools to contribute your voices to create a diverse open data repository of Indian voices to this end. This data will be available in an open manner for all to use. It is a resource that allows people to build applications that leverage speech recognition.","title":"Introduction"},{"location":"#open-source","text":"Open sourcing the speech recognition technology empowers us to bring our languages to the core of our fundamentals. Vakyansh aims to open source the speech recognition models in various languages, the datasets collected through various channels and the linguistic utilities developed to process and clean the data and make it usable by speech recognition tools. The open source strategy will enable the various language communities, individuals and technologists, who are passionate about their language, to develop speech recognition applications, and integrate them in various domains for the good of the community.","title":"Open Source"},{"location":"#getting-started","text":"Vakyansh's developer documentation is meant for its adopters, developers and contributors. It would enable people to innovate and improve and build Speech Recognition Models in any language.","title":"Getting started"},{"location":"#modeling-process","text":"","title":"Modeling Process"},{"location":"#vakyansh-components","text":"","title":"Vakyansh Components"},{"location":"#1-data-collection-pipeline","text":"Pipelines to collect data in automated way for the language you want","title":"1. Data Collection Pipeline"},{"location":"#2-intelligent-data-pipeline","text":"Pipelines to transform raw data and prepare data for model training. They clean, process and balance data for model training","title":"2. Intelligent Data Pipeline"},{"location":"#3-model-training-pipeline","text":"Pipeline to build state of the art Speech Recognition Model using the data provided by Intelligent Data Pipeline","title":"3. Model Training Pipeline"},{"location":"#vakyansh-technical-overview","text":"The Developer documentation provides you with a complete set of guidelines which you need to: Install Vakyansh Configure Vakyansh Customize Vakyansh Extend Vakyansh Contribute to Vakyansh","title":"Vakyansh Technical Overview"},{"location":"CONTRIBUTING/","text":"Contributing to this project Contributors Agreement Introduction I don't want to read this whole thing I just have a question!!! How Can I Contribute? Reporting Bugs Suggesting Enhancements Creating Pull Requests Contributors Agreement By submitting patches to this project you agree to allow them to be redistributed under the project's license, according to the normal forms and usages of the open-source community. Introduction First off, thank you for considering contributing to this project. It's people like you that make it such a great tool. Following these guidelines helps to communicate that you respect the time of the developers managing and developing this open source project. In return, they should reciprocate that respect in addressing your issue, assessing changes, and helping you finalize your pull requests. This is an open source project and we love to receive contributions from our community \u2014 you! There are many ways to contribute, from writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests or writing code which can be incorporated into the main project itself. I don't want to read this whole thing I just have a question!!! We currently allow our users to use the issue tracker for support questions. But please be wary that maintaining an open source project can take a lot of time from the maintainers. If asking for a support question, state it clearly and take the time to explain your problem properly. Also, if your problem is not strictly related to this project we recommend you to use Stack Overlow instead. How Can I Contribute? Reporting Bugs Before creating bug reports, please check the existing bug reports as you might find out that you don't need to create one. When you are creating a bug report, please include as many details as possible. How Do I Submit A (Good) Bug Report? Bugs are tracked as GitHub issues . Create an issue on the project's repository and provide the following information. Explain the problem and include additional details to help maintainers reproduce the problem: Use a clear and descriptive title for the issue to identify the problem. Describe the exact steps which reproduce the problem in as many details as possible. For example, start by explaining how you used the project. When listing steps, don't just say what you did, but explain how you did it . Provide specific examples to demonstrate the steps . It's always better to get more information. You can include links to files or GitHub projects, copy/pasteable snippets or even print screens or animated GIFS. If you're providing snippets in the issue, use Markdown code blocks . Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior. Explain which behavior you expected to see instead and why. If the problem wasn't triggered by a specific action , describe what you were doing before the problem happened and share more information using the guidelines below. Provide more context by answering these questions: Did the problem start happening recently (e.g. after updating to a new version) or was this always a problem? If the problem started happening recently, can you reproduce the problem in an older version? What's the most recent version in which the problem doesn't happen? Can you reliably reproduce the issue? If not, provide details about how often the problem happens and under which conditions it normally happens. Include details about your configuration and environment: Which version of the project are you using? What's the name and version of the OS you're using ? Any other information that could be useful about you environment Suggesting Enhancements This section guides you through submitting an enhancement suggestion for this project, including completely new features and minor improvements to existing functionality. Following these guidelines helps maintainers and the community understand your suggestion and find related suggestions. Before creating enhancement suggestions, please check the list of enhancements suggestions in the issue tracker as you might find out that you don't need to create one. When you are creating an enhancement suggestion, please include as many details as possible. How Do I Submit A (Good) Enhancement Suggestion? Enhancement suggestions are tracked as GitHub issues . Create an issue on the project's repository and provide the following information: Use a clear and descriptive title for the issue to identify the suggestion. Provide a step-by-step description of the suggested enhancement in as many details as possible. Provide specific examples to demonstrate the steps . It's always better to get more information. You can include links to files or GitHub projects, copy/pasteable snippets or even print screens or animated GIFS. If you're providing snippets in the issue, use Markdown code blocks . Describe the current behavior and explain which behavior you expected to see instead and why. List some other similar projects where this enhancement exists. Specify which version of the project you're using. Specify the current environment you're using. if this is a useful information. Creating Pull Requests How Do I Submit A (Good) Pull Request? Be warned that the contributors agreement displayed on top of this document is applicable as soon as you create a pull request . Use a clear and descriptive title for the pull request to state the improvement you made to the code or the bug you solved. Provide a link to the related issue if the pull request is a follow up of an existing bug report or enhancement suggestion. Comment why this pull request represents an enhancement and give a rationale explaining why you did it that way and not another way. Use the same coding style than the one used in this project . Welcome suggestions from the maintainers to improve your pull request .","title":"Contributions"},{"location":"CONTRIBUTING/#contributing-to-this-project","text":"Contributors Agreement Introduction I don't want to read this whole thing I just have a question!!! How Can I Contribute? Reporting Bugs Suggesting Enhancements Creating Pull Requests","title":"Contributing to this project"},{"location":"CONTRIBUTING/#contributors-agreement","text":"By submitting patches to this project you agree to allow them to be redistributed under the project's license, according to the normal forms and usages of the open-source community.","title":"Contributors Agreement"},{"location":"CONTRIBUTING/#introduction","text":"First off, thank you for considering contributing to this project. It's people like you that make it such a great tool. Following these guidelines helps to communicate that you respect the time of the developers managing and developing this open source project. In return, they should reciprocate that respect in addressing your issue, assessing changes, and helping you finalize your pull requests. This is an open source project and we love to receive contributions from our community \u2014 you! There are many ways to contribute, from writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests or writing code which can be incorporated into the main project itself.","title":"Introduction"},{"location":"CONTRIBUTING/#i-dont-want-to-read-this-whole-thing-i-just-have-a-question","text":"We currently allow our users to use the issue tracker for support questions. But please be wary that maintaining an open source project can take a lot of time from the maintainers. If asking for a support question, state it clearly and take the time to explain your problem properly. Also, if your problem is not strictly related to this project we recommend you to use Stack Overlow instead.","title":"I don't want to read this whole thing I just have a question!!!"},{"location":"CONTRIBUTING/#how-can-i-contribute","text":"","title":"How Can I Contribute?"},{"location":"CONTRIBUTING/#reporting-bugs","text":"Before creating bug reports, please check the existing bug reports as you might find out that you don't need to create one. When you are creating a bug report, please include as many details as possible.","title":"Reporting Bugs"},{"location":"CONTRIBUTING/#how-do-i-submit-a-good-bug-report","text":"Bugs are tracked as GitHub issues . Create an issue on the project's repository and provide the following information. Explain the problem and include additional details to help maintainers reproduce the problem: Use a clear and descriptive title for the issue to identify the problem. Describe the exact steps which reproduce the problem in as many details as possible. For example, start by explaining how you used the project. When listing steps, don't just say what you did, but explain how you did it . Provide specific examples to demonstrate the steps . It's always better to get more information. You can include links to files or GitHub projects, copy/pasteable snippets or even print screens or animated GIFS. If you're providing snippets in the issue, use Markdown code blocks . Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior. Explain which behavior you expected to see instead and why. If the problem wasn't triggered by a specific action , describe what you were doing before the problem happened and share more information using the guidelines below. Provide more context by answering these questions: Did the problem start happening recently (e.g. after updating to a new version) or was this always a problem? If the problem started happening recently, can you reproduce the problem in an older version? What's the most recent version in which the problem doesn't happen? Can you reliably reproduce the issue? If not, provide details about how often the problem happens and under which conditions it normally happens. Include details about your configuration and environment: Which version of the project are you using? What's the name and version of the OS you're using ? Any other information that could be useful about you environment","title":"How Do I Submit A (Good) Bug Report?"},{"location":"CONTRIBUTING/#suggesting-enhancements","text":"This section guides you through submitting an enhancement suggestion for this project, including completely new features and minor improvements to existing functionality. Following these guidelines helps maintainers and the community understand your suggestion and find related suggestions. Before creating enhancement suggestions, please check the list of enhancements suggestions in the issue tracker as you might find out that you don't need to create one. When you are creating an enhancement suggestion, please include as many details as possible.","title":"Suggesting Enhancements"},{"location":"CONTRIBUTING/#how-do-i-submit-a-good-enhancement-suggestion","text":"Enhancement suggestions are tracked as GitHub issues . Create an issue on the project's repository and provide the following information: Use a clear and descriptive title for the issue to identify the suggestion. Provide a step-by-step description of the suggested enhancement in as many details as possible. Provide specific examples to demonstrate the steps . It's always better to get more information. You can include links to files or GitHub projects, copy/pasteable snippets or even print screens or animated GIFS. If you're providing snippets in the issue, use Markdown code blocks . Describe the current behavior and explain which behavior you expected to see instead and why. List some other similar projects where this enhancement exists. Specify which version of the project you're using. Specify the current environment you're using. if this is a useful information.","title":"How Do I Submit A (Good) Enhancement Suggestion?"},{"location":"CONTRIBUTING/#creating-pull-requests","text":"","title":"Creating Pull Requests"},{"location":"CONTRIBUTING/#how-do-i-submit-a-good-pull-request","text":"Be warned that the contributors agreement displayed on top of this document is applicable as soon as you create a pull request . Use a clear and descriptive title for the pull request to state the improvement you made to the code or the bug you solved. Provide a link to the related issue if the pull request is a follow up of an existing bug report or enhancement suggestion. Comment why this pull request represents an enhancement and give a rationale explaining why you did it that way and not another way. Use the same coding style than the one used in this project . Welcome suggestions from the maintainers to improve your pull request .","title":"How Do I Submit A (Good) Pull Request?"},{"location":"RELEASE_NOTES/","text":"","title":"Release notes"},{"location":"about/","text":"","title":"About"},{"location":"data_collection/","text":"Data Collection Pipeline Table of Contents Data Collection Pipeline Table of Contents About The Project Built With Summary Getting Started Prerequisites Installation Usage Common configuration steps: Setting credentials for Google cloud bucket Bucket configuration Metadata file configurations Youtube download configurations Youtube API configuration Web Crawl Configuration Adding new spider Running services Youtube spider in channel mode: Youtube spider in file mode: Bing Spider Urls Spider Selenium google crawler Selenium youtube crawler for file mode and api mode Contributing License Contact Acknowledgements About The Project This is downloading framework that is extensible and allows the user to add new source without much code changes. For each new source user need to write a scrapy spider script and rest of downloading and meta file creation is handled by repective pipelines. And if required user can add their custom pipelines. This framework automatically transfer the downloaded data to a Google cloud bucket automatically. For more info on writing scrapy spider and pipeline one can refer to the documentation . Smart Crawlers\u2019s developer documentation is meant for its adopters, developers and contributors. The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Smart Crawlers. Smart Crawlers is based on an open platform, you are free to use any programming language to extend or customize it but we prefer to use python to perform smart scraping. The Developer documentation provides you with a complete set of guidelines which you need to: Install Smart Crawlers Configure Smart Crawlers Customize Smart Crawlers Extend Smart Crawlers Contribute to Smart Crawlers Built With We have used scrapy as the base of this framework. * Scrapy Summary This summary mentions the key advantages and limitations of this smart crawler service. Youtube Crawler Key Points and Advantages: Get language relevant channels from YouTube and download videos from them.(70%-80% relevancy with language - based on Manual Analysis) Can fetch channels with Creative Commons video and download the videos in them as well.(70% relevancy with language) Can download using file mode(manually filled with video Ids) or channel mode. Youtube-dl can fetch N number of videos from a channel and download them. YouTube crawler downloads files at a rate of maximum of 2000 hours per day and minimum of 800 hours per day. Youtube crawler is more convenient and it\u2019s a main source of Creative Commons data that can be accessed easily. It can be deployed in cloud service called zyte used for scraping/crawling. License information of videos are available in metadata. Limitations: Youtube-api cannot return more than 500 videos per channel.(when using YOUTUBE_API mode in configuration) Youtube-api is restricted to 10000 tokens per day in free mode. 10000 tokens can be used to get license info of 10000 videos.(in any mode) 10000 tokens can be used to get 5000 channels.(in YOUTUBE_API mode) Youtube-dl can be used to get all videos freely.(in YOUTUBE_DL mode) Cannot fetch data from specific playlist. (Solution: Fetch videos Ids of a playlist using YouTube-dl and put them in a file and download in file mode.) Rare cases in which you might get Too many requests error from Youtube-DL. (Solution: Rerun the application with same sources.) Cannot download videos which require user information and private videos. Web Crawler Key Points and Advantages: Web crawler can download specific language audio but with around 50 - 60% relevance. Web crawler downloads files at a rate of at least 2000 hours per day. It is a faster means of downloading data. Creative Commons license of videos can be identified if available while crawling websites. Limitations: Web crawler is not finely tuned yet, so downloaded content might have low language relevance. It cannot be deployed in zyte service free accounts and can be only deployed in zyte service paid accounts where docker container creation can be customised. License information of videos in web crawler cannot be automatically identified but requires some manual intervention. Getting Started To get started install the prerequisites and clone the repo to machine on which you wish to run the framework. Prerequisites Install ffmpeg library using commands mentioned below. For any linux based operating system (preferred Ubuntu): sudo apt-get install ffmpeg For Mac-os: brew install ffmpeg Windows user can follow installation steps on https://www.ffmpeg.org Install Python Version = 3.6 Get credentials from google developer console for google cloud storage access. Installation Clone the repo using git clone https://github.com/Open-Speech-EkStep/data-acquisition-pipeline.git Go inside the directory cd data-acquisition-pipeline Install python requirements pip install -r requirements.txt Usage This framework allows the user to download the media file from a websource(youtube, xyz.com, etc) and creates the respective metadata file from the data that is extracted from the file.For using any added source or to add new source refer to steps below.It can also crawl internet for media of a specific language. For web crawling, refer to the web crawl configuration below. Common configuration steps: Setting credentials for Google cloud bucket You can set credentials for Google cloud bucket in the credentials.json add the credentials in given manner It can be found in the project root folder. {\"Credentials\":{ YOUR ACCOUNT CREDENTIAL KEYS }} Note: All configuration files can be found in the following path data-acquisition-pipeline/data_acquisition_framework/configs/ Bucket configuration Bucket configurations for data transfer in storage_config.json \"bucket\": \"ekstepspeechrecognition-dev\", Your bucket name \"channel_blob_path\": \"scrapydump/refactor_test\", Path to directory where downloaded files is to be stored \"archive_blob_path\": \"archive\", Folder name in which history of download is to be maintained \"channels_file_blob_path\": \"channels\", Folder name in which channels and its videos are saved \"scraped_data_blob_path\": \"data_to_be_scraped\" Folder name in which CSV for youtube file mode is stored Note: 1. The scraped_data_blob_path folder should be present inside the channel_blob_path folder. 2. The CSV file used in file mode of youtube and its name must be same as source_name given above. 3. (only for datacollector_urls and datacollector_bing spiders) To autoconfigure language parameter to channel_blob_path from web_crawler_config.json, use <language> in channel_blob_path. \"eg: for tamil : data/download/<language>/audio - this will replace <language> with tamil.\" 4. The archive_blob_path and channels_file_blob_path are folders that will be autogenerated in bucket with the given name. Metadata file configurations Metadata file configurations in config.json mode: 'complete' This should not be changed audio_id: null If you want to give a custom audio id add here cleaned_duration: null If you know the cleaned duration of audio add here num_of_speakers: null Number of speaker present in audio language: Hindi Language of audio has_other_audio_signature: False If audio has multiple speaker in same file (True/False) type: 'audio' Type of media (audio or video) source: 'Demo_Source' Source name experiment_use: False If its for experimental use (True/False) utterances_files_list: null source_website: '' Source website url experiment_name: null Name of experiment if experiment_use is True mother_tongue: null Accent of language(Bengali, Marathi, etc...) age_group: null Age group of speaker in audio recorded_state: null State in which audio is recorded recorded_district: null District of state in which audio is recorded recorded_place: null Recording location recorded_date: null Recording date purpose: null Purpose of recording speaker_gender: null Gender of speaker speaker_name: null Name of speaker Note: 1. If any of the field info is not available keep its value to null 2. If speaker_name or speaker_gender is given then that same will be used for all the files in given source Youtube download configurations You can set download mode [file/channel] in youtube_pipeline_config.py mode = 'file' # [channel,file] In file mode you will store a csv file whose name must be same as source name in scraped_data_blob_path. csv must contain urls of youtube videos, speaker name and gender as three different columns. Urls is a must field. You can leave speaker name and gender blank if data is not available. Given below is the structure of csv. video_url,speaker_name,speaker_gender https://www.youtube.com/watch?v=K1vW_ZikA5o,Ram_Singh,male https://www.youtube.com/watch?v=o82HIOgozi8,John_Doe,male ... Common configurations in youtube_pipeline_config.py # Common configurations \"source_name\": \"DEMO\", This is the name of source you are downloading batch_num = 1 Number of videos to be downloaded as batches youtube_service_to_use = YoutubeService.YOUTUBE_DL This field is to choose which service to use for getting video information only_creative_commons = False Should Download only creative commons(True, False) Possible values for youtube_service_to_use: (YoutubeService.YOUTUBE_DL, YoutubeService.YOUTUBE_API) * File mode configurations in youtube_pipeline_config.py # File Mode configurations file_speaker_gender_column = 'speaker_gender' Gender column name in csv file file_speaker_name_column = \"speaker_name\" Speaker name column name in csv file file_url_name_column = \"video_url\" Video url column name in csv file license_column = \"license\" Video license column name in csv file channel mode configuration in youtube_pipeline_config.py # Channel mode configurations channel_url_dict = {} Channel url dictionary (This will download all the videos from the given channels with corresponding source names) Note: 1. In channel_url_dict, the keys must be the urls and values must be their channel names 2. To get list of channels from youtube API, channel_url_dict must be empty Youtube API configuration Automated Youtube fetching configuration in youtube_api_config.json # Youtube API configurations \"language\" : \"hindi\", Type of language for which search results are required. \"language_code\": \"hi\", Language code for the specified language. \"keywords\":[ The search keywords to be given in youtube API query \"audio\", \"speech\", \"talk\" ], \"words_to_ignore\":[ The words that are to be ignored in youtube API query \"song\", \"music\" ], \"max_results\": 20 Maximum number of channels or results that is required. Web Crawl Configuration web crawl configuration in web_crawl_config.json (Use this only for datacollector_bing and datacollector_urls spider) \"language\": \"gujarati\", Language to be crawled \"language_code\": \"gu\", Language code for the specified language. \"keywords\": [ Keywords to query \"talks audio\", \"audiobooks\", \"speeches\", ], \"word_to_ignore\": [ Words to ignore while crawling \"ieeexplore.ieee.org\", \"dl.acm.org\", \"www.microsoft.com\" ], \"extensions_to_ignore\": [ Formats/extensions to ignore while crawling \".jpeg\", \"xlsx\", \".xml\" ], \"extensions_to_include\": [ Formats/extensions to include while crawling \".mp3\", \".wav\", \".mp4\", ], \"pages\": 1, Number of pages to crawl \"depth\": 1, Nesting depth for each website \"continue_page\": \"NO\", Field to continue/resume crawling \"last_visited\": 200, Last visited results count \"enable_hours_restriction\": \"YES\", Restrict crawling based on hours of data collected \"max_hours\": 1 Maximum hours to crawl Adding new spider As we already mentioned our framework is extensible for any new source. To add a new source user just need to write a spider for that source. To add a spider you can follow the scrapy documentation or you can check our sample spider. Running services Make sure the google credentials are present in project root folder in credentials.json file. Youtube spider in channel mode: In data_acqusition_framework/configs , do the following: Open config.json and change language and type to your respective use case. Open storage_config.json and change bucket and channel_blob_path to your respective gcp paths.(For more info on these fields, scroll above to Bucket configuration) Open youtube_pipeline_config.py and change mode to channel (eg: mode='channel') There are two ways to download videos of youtube channels: You can hardcode the channel url and channel name. You can use youtube-utils service(youtube-dl/youtube data api) to fetch channels and its respective videos information. To download by hardcoding the channel urls, do the following: Open data_acqusition_framework/configs/youtube_pipeline_config.py and do the following: Add the channel_urls and its names in channel_url_dict variable. eg. channel_url_dict = { \"https://www.youtube.com/channel/UC2XEzs5R1mn2wTKgtjuMxiQ\": \"channel_name_a\", \"https://www.youtube.com/channel/UC2XEzs5R1mn2wTKgtjuMxiQ\":\"channel_name_b\" } Set youtube_service_to_use variable value to either YoutubeService.YOUTUBE_DL or YoutubeService.YOUTUBE_API for collecting video info. If YoutubeService.YOUTUBE_API is chosen, then get APIKEY for youtube data api from google developer console and store it in a file called .youtube_api_key in project root folder. From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos from youtube for the given channels and download them to bucket. To download by using youtube-utils service, do the following: Open data_acqusition_framework/configs/youtube_pipeline_config.py and do the following: Assign channel_url_dict = {} (If not empty, will not work). Set youtube_service_to_use variable value to either YoutubeService.YOUTUBE_DL or YoutubeService.YOUTUBE_API for collecting video info. If YoutubeService.YOUTUBE_API is chosen, then get APIKEY for youtube data api from google developer console and store it in a file called .youtube_api_key in project root folder. Open data_acqusition_framework/configs/youtube_api_config.json and change the fields to your requirements.(For more info: check above in Youtube api configuration) From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos from youtube for the given channels and download them to bucket. Youtube spider in file mode: In data_acqusition_framework/configs , do the following: Open config.json and change language and type to your respective use case. Open storage_config.json and change bucket and channel_blob_path to your respective gcp paths.(For more info on these fields, scroll above to Bucket configuration) Open youtube_pipeline_config.py and do the following: change mode to file (eg: mode='file'). change source_name to your requirement so that videos get downloaded to that folder in google storage bucket. Next Steps: Create a file in the following format: eg. source_name.csv with content (license column is optional): Here source_name in source_name.csv is the name you gave in youtube_pipeline_config.py file. It should be the same. video_url,speaker_name,speaker_gender,license https://www.youtube.com/watch?v=K1vW_ZikA5o,Ram_Singh,male,Creative Commons https://www.youtube.com/watch?v=o82HIOgozi8,John_Doe,male,Standard Youtube ... Now to upload this file to google cloud storage do the following: Open the channel_blob_path folder that you gave in storage_config.json and create a folder there named data_to_be_scraped . Upload the file that you created with previous step to this folder. From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos mentioned in the file from youtube and download them to bucket. Bing Spider Configure data_acquisition_framework/configs/web_crawl_config.json for your requirements. Starting datacollector_bing spider with audio pipeline. From project root folder, run the following: scrapy crawl datacollector_bing Urls Spider Configure data_acquisition_framework/configs/web_crawl_config.json for your requirements. Starting datacollector_urls spider with audio pipeline. Make sure to put the urls to crawl in the data_acquisition_framework/urls.txt . From project root folder, run the following: scrapy crawl datacollector_urls Selenium google crawler It is capable of crawling search results of google for a given language and exporting them to urls.txt file. This urls.txt file can be used with datacollector_urls spider to crawl all the search results website and download the media along with their metadata. A specified Readme can be found in selenium_google_crawler folder. Readme for selenium google crawler Selenium youtube crawler for file mode and api mode It is capable of crawling youtube videos using youtube api or from a list of files with youtube video ids provided with channel name as filename. A specified Readme can be found in selenium_youtube_crawler folder. Readme for selenium youtube crawler Contributing Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request License Distributed under the [XYZ] License. See LICENSE for more information. Contact Your Name - @your_twitter - email@example.com Project Link: https://github.com/your_username/repo_name Acknowledgements Scrapy YouTube-dl TinyTag","title":"Data Collection Pipeine"},{"location":"data_collection/#data-collection-pipeline","text":"","title":"Data Collection Pipeline"},{"location":"data_collection/#table-of-contents","text":"Data Collection Pipeline Table of Contents About The Project Built With Summary Getting Started Prerequisites Installation Usage Common configuration steps: Setting credentials for Google cloud bucket Bucket configuration Metadata file configurations Youtube download configurations Youtube API configuration Web Crawl Configuration Adding new spider Running services Youtube spider in channel mode: Youtube spider in file mode: Bing Spider Urls Spider Selenium google crawler Selenium youtube crawler for file mode and api mode Contributing License Contact Acknowledgements","title":"Table of Contents"},{"location":"data_collection/#about-the-project","text":"This is downloading framework that is extensible and allows the user to add new source without much code changes. For each new source user need to write a scrapy spider script and rest of downloading and meta file creation is handled by repective pipelines. And if required user can add their custom pipelines. This framework automatically transfer the downloaded data to a Google cloud bucket automatically. For more info on writing scrapy spider and pipeline one can refer to the documentation . Smart Crawlers\u2019s developer documentation is meant for its adopters, developers and contributors. The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Smart Crawlers. Smart Crawlers is based on an open platform, you are free to use any programming language to extend or customize it but we prefer to use python to perform smart scraping. The Developer documentation provides you with a complete set of guidelines which you need to: Install Smart Crawlers Configure Smart Crawlers Customize Smart Crawlers Extend Smart Crawlers Contribute to Smart Crawlers","title":"About The Project"},{"location":"data_collection/#built-with","text":"We have used scrapy as the base of this framework. * Scrapy","title":"Built With"},{"location":"data_collection/#summary","text":"This summary mentions the key advantages and limitations of this smart crawler service. Youtube Crawler Key Points and Advantages: Get language relevant channels from YouTube and download videos from them.(70%-80% relevancy with language - based on Manual Analysis) Can fetch channels with Creative Commons video and download the videos in them as well.(70% relevancy with language) Can download using file mode(manually filled with video Ids) or channel mode. Youtube-dl can fetch N number of videos from a channel and download them. YouTube crawler downloads files at a rate of maximum of 2000 hours per day and minimum of 800 hours per day. Youtube crawler is more convenient and it\u2019s a main source of Creative Commons data that can be accessed easily. It can be deployed in cloud service called zyte used for scraping/crawling. License information of videos are available in metadata. Limitations: Youtube-api cannot return more than 500 videos per channel.(when using YOUTUBE_API mode in configuration) Youtube-api is restricted to 10000 tokens per day in free mode. 10000 tokens can be used to get license info of 10000 videos.(in any mode) 10000 tokens can be used to get 5000 channels.(in YOUTUBE_API mode) Youtube-dl can be used to get all videos freely.(in YOUTUBE_DL mode) Cannot fetch data from specific playlist. (Solution: Fetch videos Ids of a playlist using YouTube-dl and put them in a file and download in file mode.) Rare cases in which you might get Too many requests error from Youtube-DL. (Solution: Rerun the application with same sources.) Cannot download videos which require user information and private videos. Web Crawler Key Points and Advantages: Web crawler can download specific language audio but with around 50 - 60% relevance. Web crawler downloads files at a rate of at least 2000 hours per day. It is a faster means of downloading data. Creative Commons license of videos can be identified if available while crawling websites. Limitations: Web crawler is not finely tuned yet, so downloaded content might have low language relevance. It cannot be deployed in zyte service free accounts and can be only deployed in zyte service paid accounts where docker container creation can be customised. License information of videos in web crawler cannot be automatically identified but requires some manual intervention.","title":"Summary"},{"location":"data_collection/#getting-started","text":"To get started install the prerequisites and clone the repo to machine on which you wish to run the framework.","title":"Getting Started"},{"location":"data_collection/#prerequisites","text":"Install ffmpeg library using commands mentioned below. For any linux based operating system (preferred Ubuntu): sudo apt-get install ffmpeg For Mac-os: brew install ffmpeg Windows user can follow installation steps on https://www.ffmpeg.org Install Python Version = 3.6 Get credentials from google developer console for google cloud storage access.","title":"Prerequisites"},{"location":"data_collection/#installation","text":"Clone the repo using git clone https://github.com/Open-Speech-EkStep/data-acquisition-pipeline.git Go inside the directory cd data-acquisition-pipeline Install python requirements pip install -r requirements.txt","title":"Installation"},{"location":"data_collection/#usage","text":"This framework allows the user to download the media file from a websource(youtube, xyz.com, etc) and creates the respective metadata file from the data that is extracted from the file.For using any added source or to add new source refer to steps below.It can also crawl internet for media of a specific language. For web crawling, refer to the web crawl configuration below.","title":"Usage"},{"location":"data_collection/#common-configuration-steps","text":"","title":"Common configuration steps:"},{"location":"data_collection/#setting-credentials-for-google-cloud-bucket","text":"You can set credentials for Google cloud bucket in the credentials.json add the credentials in given manner It can be found in the project root folder. {\"Credentials\":{ YOUR ACCOUNT CREDENTIAL KEYS }} Note: All configuration files can be found in the following path data-acquisition-pipeline/data_acquisition_framework/configs/","title":"Setting credentials for Google cloud bucket"},{"location":"data_collection/#bucket-configuration","text":"Bucket configurations for data transfer in storage_config.json \"bucket\": \"ekstepspeechrecognition-dev\", Your bucket name \"channel_blob_path\": \"scrapydump/refactor_test\", Path to directory where downloaded files is to be stored \"archive_blob_path\": \"archive\", Folder name in which history of download is to be maintained \"channels_file_blob_path\": \"channels\", Folder name in which channels and its videos are saved \"scraped_data_blob_path\": \"data_to_be_scraped\" Folder name in which CSV for youtube file mode is stored Note: 1. The scraped_data_blob_path folder should be present inside the channel_blob_path folder. 2. The CSV file used in file mode of youtube and its name must be same as source_name given above. 3. (only for datacollector_urls and datacollector_bing spiders) To autoconfigure language parameter to channel_blob_path from web_crawler_config.json, use <language> in channel_blob_path. \"eg: for tamil : data/download/<language>/audio - this will replace <language> with tamil.\" 4. The archive_blob_path and channels_file_blob_path are folders that will be autogenerated in bucket with the given name.","title":"Bucket configuration"},{"location":"data_collection/#metadata-file-configurations","text":"Metadata file configurations in config.json mode: 'complete' This should not be changed audio_id: null If you want to give a custom audio id add here cleaned_duration: null If you know the cleaned duration of audio add here num_of_speakers: null Number of speaker present in audio language: Hindi Language of audio has_other_audio_signature: False If audio has multiple speaker in same file (True/False) type: 'audio' Type of media (audio or video) source: 'Demo_Source' Source name experiment_use: False If its for experimental use (True/False) utterances_files_list: null source_website: '' Source website url experiment_name: null Name of experiment if experiment_use is True mother_tongue: null Accent of language(Bengali, Marathi, etc...) age_group: null Age group of speaker in audio recorded_state: null State in which audio is recorded recorded_district: null District of state in which audio is recorded recorded_place: null Recording location recorded_date: null Recording date purpose: null Purpose of recording speaker_gender: null Gender of speaker speaker_name: null Name of speaker Note: 1. If any of the field info is not available keep its value to null 2. If speaker_name or speaker_gender is given then that same will be used for all the files in given source","title":"Metadata file configurations"},{"location":"data_collection/#youtube-download-configurations","text":"You can set download mode [file/channel] in youtube_pipeline_config.py mode = 'file' # [channel,file] In file mode you will store a csv file whose name must be same as source name in scraped_data_blob_path. csv must contain urls of youtube videos, speaker name and gender as three different columns. Urls is a must field. You can leave speaker name and gender blank if data is not available. Given below is the structure of csv. video_url,speaker_name,speaker_gender https://www.youtube.com/watch?v=K1vW_ZikA5o,Ram_Singh,male https://www.youtube.com/watch?v=o82HIOgozi8,John_Doe,male ... Common configurations in youtube_pipeline_config.py # Common configurations \"source_name\": \"DEMO\", This is the name of source you are downloading batch_num = 1 Number of videos to be downloaded as batches youtube_service_to_use = YoutubeService.YOUTUBE_DL This field is to choose which service to use for getting video information only_creative_commons = False Should Download only creative commons(True, False) Possible values for youtube_service_to_use: (YoutubeService.YOUTUBE_DL, YoutubeService.YOUTUBE_API) * File mode configurations in youtube_pipeline_config.py # File Mode configurations file_speaker_gender_column = 'speaker_gender' Gender column name in csv file file_speaker_name_column = \"speaker_name\" Speaker name column name in csv file file_url_name_column = \"video_url\" Video url column name in csv file license_column = \"license\" Video license column name in csv file channel mode configuration in youtube_pipeline_config.py # Channel mode configurations channel_url_dict = {} Channel url dictionary (This will download all the videos from the given channels with corresponding source names) Note: 1. In channel_url_dict, the keys must be the urls and values must be their channel names 2. To get list of channels from youtube API, channel_url_dict must be empty","title":"Youtube download configurations"},{"location":"data_collection/#youtube-api-configuration","text":"Automated Youtube fetching configuration in youtube_api_config.json # Youtube API configurations \"language\" : \"hindi\", Type of language for which search results are required. \"language_code\": \"hi\", Language code for the specified language. \"keywords\":[ The search keywords to be given in youtube API query \"audio\", \"speech\", \"talk\" ], \"words_to_ignore\":[ The words that are to be ignored in youtube API query \"song\", \"music\" ], \"max_results\": 20 Maximum number of channels or results that is required.","title":"Youtube API configuration"},{"location":"data_collection/#web-crawl-configuration","text":"web crawl configuration in web_crawl_config.json (Use this only for datacollector_bing and datacollector_urls spider) \"language\": \"gujarati\", Language to be crawled \"language_code\": \"gu\", Language code for the specified language. \"keywords\": [ Keywords to query \"talks audio\", \"audiobooks\", \"speeches\", ], \"word_to_ignore\": [ Words to ignore while crawling \"ieeexplore.ieee.org\", \"dl.acm.org\", \"www.microsoft.com\" ], \"extensions_to_ignore\": [ Formats/extensions to ignore while crawling \".jpeg\", \"xlsx\", \".xml\" ], \"extensions_to_include\": [ Formats/extensions to include while crawling \".mp3\", \".wav\", \".mp4\", ], \"pages\": 1, Number of pages to crawl \"depth\": 1, Nesting depth for each website \"continue_page\": \"NO\", Field to continue/resume crawling \"last_visited\": 200, Last visited results count \"enable_hours_restriction\": \"YES\", Restrict crawling based on hours of data collected \"max_hours\": 1 Maximum hours to crawl","title":"Web Crawl Configuration"},{"location":"data_collection/#adding-new-spider","text":"As we already mentioned our framework is extensible for any new source. To add a new source user just need to write a spider for that source. To add a spider you can follow the scrapy documentation or you can check our sample spider.","title":"Adding new spider"},{"location":"data_collection/#running-services","text":"Make sure the google credentials are present in project root folder in credentials.json file.","title":"Running services"},{"location":"data_collection/#youtube-spider-in-channel-mode","text":"In data_acqusition_framework/configs , do the following: Open config.json and change language and type to your respective use case. Open storage_config.json and change bucket and channel_blob_path to your respective gcp paths.(For more info on these fields, scroll above to Bucket configuration) Open youtube_pipeline_config.py and change mode to channel (eg: mode='channel') There are two ways to download videos of youtube channels: You can hardcode the channel url and channel name. You can use youtube-utils service(youtube-dl/youtube data api) to fetch channels and its respective videos information. To download by hardcoding the channel urls, do the following: Open data_acqusition_framework/configs/youtube_pipeline_config.py and do the following: Add the channel_urls and its names in channel_url_dict variable. eg. channel_url_dict = { \"https://www.youtube.com/channel/UC2XEzs5R1mn2wTKgtjuMxiQ\": \"channel_name_a\", \"https://www.youtube.com/channel/UC2XEzs5R1mn2wTKgtjuMxiQ\":\"channel_name_b\" } Set youtube_service_to_use variable value to either YoutubeService.YOUTUBE_DL or YoutubeService.YOUTUBE_API for collecting video info. If YoutubeService.YOUTUBE_API is chosen, then get APIKEY for youtube data api from google developer console and store it in a file called .youtube_api_key in project root folder. From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos from youtube for the given channels and download them to bucket. To download by using youtube-utils service, do the following: Open data_acqusition_framework/configs/youtube_pipeline_config.py and do the following: Assign channel_url_dict = {} (If not empty, will not work). Set youtube_service_to_use variable value to either YoutubeService.YOUTUBE_DL or YoutubeService.YOUTUBE_API for collecting video info. If YoutubeService.YOUTUBE_API is chosen, then get APIKEY for youtube data api from google developer console and store it in a file called .youtube_api_key in project root folder. Open data_acqusition_framework/configs/youtube_api_config.json and change the fields to your requirements.(For more info: check above in Youtube api configuration) From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos from youtube for the given channels and download them to bucket.","title":"Youtube spider in channel mode:"},{"location":"data_collection/#youtube-spider-in-file-mode","text":"In data_acqusition_framework/configs , do the following: Open config.json and change language and type to your respective use case. Open storage_config.json and change bucket and channel_blob_path to your respective gcp paths.(For more info on these fields, scroll above to Bucket configuration) Open youtube_pipeline_config.py and do the following: change mode to file (eg: mode='file'). change source_name to your requirement so that videos get downloaded to that folder in google storage bucket. Next Steps: Create a file in the following format: eg. source_name.csv with content (license column is optional): Here source_name in source_name.csv is the name you gave in youtube_pipeline_config.py file. It should be the same. video_url,speaker_name,speaker_gender,license https://www.youtube.com/watch?v=K1vW_ZikA5o,Ram_Singh,male,Creative Commons https://www.youtube.com/watch?v=o82HIOgozi8,John_Doe,male,Standard Youtube ... Now to upload this file to google cloud storage do the following: Open the channel_blob_path folder that you gave in storage_config.json and create a folder there named data_to_be_scraped . Upload the file that you created with previous step to this folder. From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos mentioned in the file from youtube and download them to bucket.","title":"Youtube spider in file mode:"},{"location":"data_collection/#bing-spider","text":"Configure data_acquisition_framework/configs/web_crawl_config.json for your requirements. Starting datacollector_bing spider with audio pipeline. From project root folder, run the following: scrapy crawl datacollector_bing","title":"Bing Spider"},{"location":"data_collection/#urls-spider","text":"Configure data_acquisition_framework/configs/web_crawl_config.json for your requirements. Starting datacollector_urls spider with audio pipeline. Make sure to put the urls to crawl in the data_acquisition_framework/urls.txt . From project root folder, run the following: scrapy crawl datacollector_urls","title":"Urls Spider"},{"location":"data_collection/#selenium-google-crawler","text":"It is capable of crawling search results of google for a given language and exporting them to urls.txt file. This urls.txt file can be used with datacollector_urls spider to crawl all the search results website and download the media along with their metadata. A specified Readme can be found in selenium_google_crawler folder. Readme for selenium google crawler","title":"Selenium google crawler"},{"location":"data_collection/#selenium-youtube-crawler-for-file-mode-and-api-mode","text":"It is capable of crawling youtube videos using youtube api or from a list of files with youtube video ids provided with channel name as filename. A specified Readme can be found in selenium_youtube_crawler folder. Readme for selenium youtube crawler","title":"Selenium youtube crawler for file mode and api mode"},{"location":"data_collection/#contributing","text":"Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request","title":"Contributing"},{"location":"data_collection/#license","text":"Distributed under the [XYZ] License. See LICENSE for more information.","title":"License"},{"location":"data_collection/#contact","text":"Your Name - @your_twitter - email@example.com Project Link: https://github.com/your_username/repo_name","title":"Contact"},{"location":"data_collection/#acknowledgements","text":"Scrapy YouTube-dl TinyTag","title":"Acknowledgements"},{"location":"intelligent_data_pipelines/","text":"Data pipelines Table of Contents About the Project Built With Getting Started Run on GCP (with composer) Installation Contributing License Contact Acknowledgements About The Project This is data processing framework that is extensible and allows user to add new module for any other processing. Here is the code In this project we have some module: Audio processor Speaker identification Gender identification Audio transcription (STT) The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Data Pipelines. * Audio processor In audio processor we run Vad to break down audio in utterances after that we run WADASNR to calculate SNR for each utterance. * Speaker identification This module is for identity the number of speaker for a given source. we are using resemblyzer. * Gender identification This module is for identity the gender of speaker in a utterance. * Audio transcription (STT) This module is for run Speech To Text (STT) in given source. we can use google,azure and we can also add more API for running STT. Getting Started To get started install the prerequisites and clone the repo to machine on which you wish to run the framework. Installation Clone the repo git clone git@github.com:Open-Speech-EkStep/audio-to-speech-pipeline.git Install python requirements pip install -r requirements.txt Run on GCP (with composer) ### Requirements: 1.Terraform https://www.terraform.io/downloads.html 2.gcloud https://cloud.google.com/sdk/docs/install ### Infra Setup (with config): Clone the repo sh git clone git@github.com:Open-Speech-EkStep/audio-to-speech-pipeline.git 2.Initialize terraform modules terraform init Select a workspace as per the environments(dev,test,prod). terraform workspace select <env_name> eg: terraform workspace select prod Run specific modules as per requirements. terraform apply -target=module.<module-name> eg: terraform apply -target=module.sql-database Run all modules at once. terraform apply CI/CD setup: Once we deploy using circle-ci it will create schema for our DB and upload our dags in airflow dag bucket that we create using terraform. and also create and push image of code in google container registry and upload our variable to airflow. ### Audio Processing (with config): description: config: steps to run : 1. We have to configure *sourcepathforsnr* in airflow variable where our raw data stored. 2. Other variable is *snrcatalogue* in that we update our source which we want to run and count how many file should run in one trigger.and format is what raw audio file format in bucket and language and parallelism is how many pod will up in one run if parallelism is not define number of pod = count ex: ```\"snrcatalogue\": { \"<source_name>\": { \"count\": 5, \"format\": \"mp3\", \"language\": \"telugu\", \"parallelism\":2 }``` 3. We have to also set *audiofilelist* with whatever source we want to run with empty array that will store our file path ex: ``` \"audiofilelist\": { \"<source_name>\": [] }``` 4. That will create a dag with the source_name now we can trigger that dag that will process given number(count) of file. and upload processed file to *remote_processed_audio_file_path* that we mentioned in config file. and move raw data from *remote_raw_audio_file_path* to *snr_done_folder_path*. and update DB also with the metadata which we created using circle-ci. ### Audio Analysis (with config) ### Data Balancing (with config): ### Audio Transcription (with config) Contributing Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request License Distributed under the [XYZ] License. See LICENSE for more information. Contact Your Name - @your_twitter - email@example.com Project Link: https://github.com/your_username/repo_name","title":"Intelligent Data Pipeline"},{"location":"intelligent_data_pipelines/#data-pipelines","text":"","title":"Data pipelines"},{"location":"intelligent_data_pipelines/#table-of-contents","text":"About the Project Built With Getting Started Run on GCP (with composer) Installation Contributing License Contact Acknowledgements","title":"Table of Contents"},{"location":"intelligent_data_pipelines/#about-the-project","text":"This is data processing framework that is extensible and allows user to add new module for any other processing. Here is the code In this project we have some module: Audio processor Speaker identification Gender identification Audio transcription (STT) The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Data Pipelines.","title":"About The Project"},{"location":"intelligent_data_pipelines/#audio-processor","text":"In audio processor we run Vad to break down audio in utterances after that we run WADASNR to calculate SNR for each utterance.","title":"* Audio processor"},{"location":"intelligent_data_pipelines/#speaker-identification","text":"This module is for identity the number of speaker for a given source. we are using resemblyzer.","title":"* Speaker identification"},{"location":"intelligent_data_pipelines/#gender-identification","text":"This module is for identity the gender of speaker in a utterance.","title":"* Gender identification"},{"location":"intelligent_data_pipelines/#audio-transcription-stt","text":"This module is for run Speech To Text (STT) in given source. we can use google,azure and we can also add more API for running STT.","title":"* Audio transcription (STT)"},{"location":"intelligent_data_pipelines/#getting-started","text":"To get started install the prerequisites and clone the repo to machine on which you wish to run the framework.","title":"Getting Started"},{"location":"intelligent_data_pipelines/#installation","text":"Clone the repo git clone git@github.com:Open-Speech-EkStep/audio-to-speech-pipeline.git Install python requirements pip install -r requirements.txt","title":"Installation"},{"location":"intelligent_data_pipelines/#run-on-gcp-with-composer","text":"### Requirements: 1.Terraform https://www.terraform.io/downloads.html 2.gcloud https://cloud.google.com/sdk/docs/install ### Infra Setup (with config): Clone the repo sh git clone git@github.com:Open-Speech-EkStep/audio-to-speech-pipeline.git 2.Initialize terraform modules terraform init Select a workspace as per the environments(dev,test,prod). terraform workspace select <env_name> eg: terraform workspace select prod Run specific modules as per requirements. terraform apply -target=module.<module-name> eg: terraform apply -target=module.sql-database Run all modules at once. terraform apply CI/CD setup: Once we deploy using circle-ci it will create schema for our DB and upload our dags in airflow dag bucket that we create using terraform. and also create and push image of code in google container registry and upload our variable to airflow. ### Audio Processing (with config): description: config: steps to run : 1. We have to configure *sourcepathforsnr* in airflow variable where our raw data stored. 2. Other variable is *snrcatalogue* in that we update our source which we want to run and count how many file should run in one trigger.and format is what raw audio file format in bucket and language and parallelism is how many pod will up in one run if parallelism is not define number of pod = count ex: ```\"snrcatalogue\": { \"<source_name>\": { \"count\": 5, \"format\": \"mp3\", \"language\": \"telugu\", \"parallelism\":2 }``` 3. We have to also set *audiofilelist* with whatever source we want to run with empty array that will store our file path ex: ``` \"audiofilelist\": { \"<source_name>\": [] }``` 4. That will create a dag with the source_name now we can trigger that dag that will process given number(count) of file. and upload processed file to *remote_processed_audio_file_path* that we mentioned in config file. and move raw data from *remote_raw_audio_file_path* to *snr_done_folder_path*. and update DB also with the metadata which we created using circle-ci. ### Audio Analysis (with config) ### Data Balancing (with config): ### Audio Transcription (with config)","title":"Run on GCP (with composer)"},{"location":"intelligent_data_pipelines/#contributing","text":"Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request","title":"Contributing"},{"location":"intelligent_data_pipelines/#license","text":"Distributed under the [XYZ] License. See LICENSE for more information.","title":"License"},{"location":"intelligent_data_pipelines/#contact","text":"Your Name - @your_twitter - email@example.com Project Link: https://github.com/your_username/repo_name","title":"Contact"},{"location":"model_training/","text":"","title":"Model Training Pipeline"}]}